a:2:{s:7:"current";a:9:{s:4:"date";a:2:{s:7:"created";i:1601022834;s:8:"modified";i:1603714498;}s:7:"creator";s:14:"Pierre Virgaux";s:4:"user";s:13:"pierrevirgaux";s:11:"last_change";a:8:{s:4:"date";i:1603714498;s:2:"ip";s:12:"88.124.10.73";s:4:"type";s:1:"E";s:2:"id";s:48:"cpp:les_foret_de_classification_et_de_regression";s:4:"user";s:13:"pierrevirgaux";s:3:"sum";s:17:"[Gradient boost] ";s:5:"extra";s:0:"";s:10:"sizechange";i:71;}s:11:"contributor";a:1:{s:13:"pierrevirgaux";s:14:"Pierre Virgaux";}s:8:"relation";a:3:{s:10:"references";a:1:{s:6:"cpp:ia";b:1;}s:5:"media";a:4:{s:24:"cpp:methode_ensemble.jpg";b:1;s:23:"cpp:forets_principe.png";b:1;s:15:"cpp:blender.jpg";b:1;s:23:"cpp:boostersonarbre.png";b:1;}s:10:"firstimage";s:24:"cpp:methode_ensemble.jpg";}s:5:"title";s:23:"Classificateur par vote";s:11:"description";a:2:{s:15:"tableofcontents";a:8:{i:0;a:4:{s:3:"hid";s:23:"classificateur_par_vote";s:5:"title";s:23:"Classificateur par vote";s:4:"type";s:2:"ul";s:5:"level";i:2;}i:1;a:4:{s:3:"hid";s:25:"faire_varier_l_algorithme";s:5:"title";s:25:"Faire varier l'algorithme";s:4:"type";s:2:"ul";s:5:"level";i:3;}i:2;a:4:{s:3:"hid";s:24:"le_bagging_et_le_pasting";s:5:"title";s:24:"Le bagging et le pasting";s:4:"type";s:2:"ul";s:5:"level";i:3;}i:3;a:4:{s:3:"hid";s:21:"les_forets_aleatoires";s:5:"title";s:23:"Les forêts aléatoires";s:4:"type";s:2:"ul";s:5:"level";i:3;}i:4;a:4:{s:3:"hid";s:11:"le_stacking";s:5:"title";s:11:"Le stacking";s:4:"type";s:2:"ul";s:5:"level";i:3;}i:5;a:4:{s:3:"hid";s:18:"booster_son_modele";s:5:"title";s:19:"Booster son modèle";s:4:"type";s:2:"ul";s:5:"level";i:2;}i:6;a:4:{s:3:"hid";s:8:"adaboost";s:5:"title";s:8:"Adaboost";s:4:"type";s:2:"ul";s:5:"level";i:3;}i:7;a:4:{s:3:"hid";s:14:"gradient_boost";s:5:"title";s:14:"Gradient boost";s:4:"type";s:2:"ul";s:5:"level";i:3;}}s:8:"abstract";s:514:"Machine Learning



L'un des plus gros problèmes des arbres de décision est leur instabilité. Une petite variation des données entraîne une grande variation au niveau du modèle. Il existe plusieurs algorithmes pour entraîner des modèles et combiner les résultats. Il s'agit d'algorithmes efficaces qui reviennent souvent dans les concours de Data Science comme meilleur modèles.$\frac{1}{m}$$$\tau_{j} = \frac{Somme \, des \, poids \, des \, observations \, males \, prédites}{Somme \, des \, poids \…";}s:8:"internal";a:2:{s:5:"cache";b:1;s:3:"toc";b:1;}}s:10:"persistent";a:5:{s:4:"date";a:2:{s:7:"created";i:1601022834;s:8:"modified";i:1603714498;}s:7:"creator";s:14:"Pierre Virgaux";s:4:"user";s:13:"pierrevirgaux";s:11:"last_change";a:8:{s:4:"date";i:1603714498;s:2:"ip";s:12:"88.124.10.73";s:4:"type";s:1:"E";s:2:"id";s:48:"cpp:les_foret_de_classification_et_de_regression";s:4:"user";s:13:"pierrevirgaux";s:3:"sum";s:17:"[Gradient boost] ";s:5:"extra";s:0:"";s:10:"sizechange";i:71;}s:11:"contributor";a:1:{s:13:"pierrevirgaux";s:14:"Pierre Virgaux";}}}