a:1060:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:0;}i:2;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:22:":cpp:titre_reg_log.png";i:1;s:22:"Régression logistique";i:2;N;i:3;s:3:"500";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:1;}i:3;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:54;}i:4;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:25:"La régression logistique";i:1;i:2;i:2;i:56;}i:2;i:56;}i:5;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:56;}i:6;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:56;}i:7;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:204:"La régression logistique est utilisée pour estimer la probabilité, qu'une observation appartienne à une classe
donnée. Considérons pour cela un cas d'application : l'attribution de crédit bancaire.";}i:2;i:93;}i:8;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:297;}i:9;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:297;}i:10;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:375:"Le classificateur va retourner la probabilité que Marie, 65 ans et retraitée, obtienne un crédit. Dans le cas où cette probabilité est supérieure ou égale à 50 %, le modèle prédit que Marie aura son crédit. Sinon 
elle ne l'a pas.  Cet aspect explicatif rend la régression logistique très populaire dans les domaines de la santé, bancaire ou encore ingénierie.";}i:2;i:299;}i:11;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:674;}i:12;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:674;}i:13;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:13:"alert-warning";}i:2;i:1;i:3;s:15:"<alert warning>";}i:2;i:676;}i:14;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:691;}i:15;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:692;}i:16;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Attention :";}i:2;i:694;}i:17;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:705;}i:18;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:40:"  Malgré le fait qu'elle soit appelée ";}i:2;i:3;i:3;s:40:"  Malgré le fait qu'elle soit appelée ";}i:2;i:707;}i:19;a:3:{i:0;s:18:"doublequoteopening";i:1;a:0:{}i:2;i:747;}i:20;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:11:"régression";}i:2;i:3;i:3;s:11:"régression";}i:2;i:748;}i:21;a:3:{i:0;s:18:"doublequoteclosing";i:1;a:0:{}i:2;i:759;}i:22;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:90:", la régression logistique est en réalité utilisée pour des travaux de classification.";}i:2;i:3;i:3;s:90:", la régression logistique est en réalité utilisée pour des travaux de classification.";}i:2;i:760;}i:23;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:850;}i:24;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:858;}i:25;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:858;}i:26;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:123:"Il existe deux extensions de régression logistique, dont la particularité réside dans le nombre de classes à prédire :";}i:2;i:860;}i:27;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:983;}i:28;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:983;}i:29;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:983;}i:30;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:983;}i:31;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:988;}i:32;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:989;}i:33;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:41:"La régression multinomiale ou softmax : ";}i:2;i:991;}i:34;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:1032;}i:35;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:93:" Cas de régression logistique où la variable cible a plus de deux classes non ordonnées  (";}i:2;i:1034;}i:36;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:20:"typography_fontcolor";i:1;a:2:{i:0;i:1;i:1;a:1:{s:12:"declarations";a:1:{s:5:"color";s:7:"#ff0000";}}}i:2;i:1;i:3;s:12:"<fc #ff0000>";}i:2;i:1127;}i:37;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"Ex :";}i:2;i:1139;}i:38;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:20:"typography_fontcolor";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:5:"</fc>";}i:2;i:1143;}i:39;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:34:" Prédiction d'un type de cancer).";}i:2;i:1148;}i:40;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1182;}i:41;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1182;}i:42;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:1182;}i:43;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:1182;}i:44;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:1186;}i:45;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:1187;}i:46;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:36:"La régression logistique ordinale :";}i:2;i:1189;}i:47;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:1225;}i:48;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:89:" Cas de régression multinomiale où les classes sont reliées par une relation d'ordre (";}i:2;i:1227;}i:49;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:20:"typography_fontcolor";i:1;a:2:{i:0;i:1;i:1;a:1:{s:12:"declarations";a:1:{s:5:"color";s:7:"#ff0000";}}}i:2;i:1;i:3;s:12:"<fc #ff0000>";}i:2;i:1316;}i:50;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"Ex :";}i:2;i:1328;}i:51;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:20:"typography_fontcolor";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:5:"</fc>";}i:2;i:1332;}i:52;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:39:" Stades d'avancement d'une épidémie).";}i:2;i:1337;}i:53;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:1376;}i:54;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:1376;}i:55;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:1376;}i:56;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1379;}i:57;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:28:"Estimation des probabilités";i:1;i:3;i:2;i:1379;}i:2;i:1379;}i:58;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:1379;}i:59;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1379;}i:60;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:79:"Le modèle de régression logistique estime la probabilité qu'une observation ";}i:2;i:1417;}i:61;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:1496;}i:62;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:5:"X_{i}";i:2;i:3;i:3;s:5:"X_{i}";}i:2;i:1497;}i:63;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:1502;}i:64;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:80:", appartienne à une classe particulière. Mais alors comment cela est-il fait ?";}i:2;i:1503;}i:65;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1583;}i:66;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1583;}i:67;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:190:"L'estimation se fait comme dans une régression linéaire, où le modèle calcule la somme pondérée des caractéristiques d'entrée, mais au lieu de retourner une valeur continue comme en ";}i:2;i:1585;}i:68;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:29:"regression_supervisee#theorie";i:1;s:21:"régression linéaire";}i:2;i:1775;}i:69;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:27:", il fournit la logistique ";}i:2;i:1830;}i:70;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:1857;}i:71;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"p";i:2;i:3;i:3;s:1:"p";}i:2;i:1858;}i:72;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:1859;}i:73;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:152:" du résultat. C'est-à-dire qu'il passe l'expression de la régression linéaire dans la fonction logistique, qui va renvoyer des valeurs entre 0 et 1.";}i:2;i:1860;}i:74;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2012;}i:75;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2012;}i:76;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:2:"$$";i:2;i:1;i:3;s:2:"$$";}i:2;i:2014;}i:77;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:41:"p = h_{\theta}(x) = \sigma(\theta.x^{T})$";i:2;i:3;i:3;s:41:"p = h_{\theta}(x) = \sigma(\theta.x^{T})$";}i:2;i:2016;}i:78;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:2057;}i:79;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2058;}i:80;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2058;}i:81;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Avec :";}i:2;i:2060;}i:82;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2066;}i:83;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:2066;}i:84;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2066;}i:85;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2066;}i:86;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:2070;}i:87;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:2071;}i:88;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:6:"\theta";i:2;i:3;i:3;s:6:"\theta";}i:2;i:2072;}i:89;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:2078;}i:90;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:34:" le vecteur paramètre du modèle.";}i:2;i:2079;}i:91;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2113;}i:92;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2113;}i:93;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2113;}i:94;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2113;}i:95;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:2117;}i:96;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:2118;}i:97;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"x";i:2;i:3;i:3;s:1:"x";}i:2;i:2119;}i:98;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:2120;}i:99;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:28:" les valeurs d'entrainement.";}i:2;i:2121;}i:100;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2149;}i:101;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2149;}i:102;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:2149;}i:103;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:2149;}i:104;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:2153;}i:105;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:2154;}i:106;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:6:"\sigma";i:2;i:3;i:3;s:6:"\sigma";}i:2;i:2155;}i:107;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:2161;}i:108;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:25:" une fonction sigmoïde. ";}i:2;i:2162;}i:109;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:2187;}i:110;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:2187;}i:111;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:2187;}i:112;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2189;}i:113;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:24:"Les fonctions sigmoïdes";i:1;i:3;i:2;i:2189;}i:2;i:2189;}i:114;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:2189;}i:115;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2189;}i:116;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:126:"La régression logistique est un estimateur probabiliste, dont la modélisation suit celle des fonctions mathématiques dites ";}i:2;i:2223;}i:117;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:2349;}i:118;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"sigmoïdes";}i:2;i:2351;}i:119;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:2361;}i:120;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:36:", caractérisées par leur forme en ";}i:2;i:2363;}i:121;a:3:{i:0;s:18:"doublequoteopening";i:1;a:0:{}i:2;i:2399;}i:122;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"S";}i:2;i:2400;}i:123;a:3:{i:0;s:18:"doublequoteclosing";i:1;a:0:{}i:2;i:2401;}i:124;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:94:". Le choix de la fonction à utiliser dépend de la loi que suivent les donnes ou du contexte.";}i:2;i:2402;}i:125;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2496;}i:126;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2498;}i:127;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:22:"La fonction logistique";i:1;i:4;i:2;i:2498;}i:2;i:2498;}i:128;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:4;}i:2;i:2498;}i:129;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2498;}i:130;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:139:"Il s'agit de la fonction de répartition de la loi logistique, utilisée dans le cadre d'une régression logistique. Elle est donnée par :";}i:2;i:2528;}i:131;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2667;}i:132;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2667;}i:133;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:2:"$$";i:2;i:1;i:3;s:2:"$$";}i:2;i:2669;}i:134;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:71:"\sigma(t) = \frac{1}{1 + e^{-t}}, \text{tel que } \sigma(t) \in [0, 1]$";i:2;i:3;i:3;s:71:"\sigma(t) = \frac{1}{1 + e^{-t}}, \text{tel que } \sigma(t) \in [0, 1]$";}i:2;i:2671;}i:135;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:2742;}i:136;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2743;}i:137;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2743;}i:138;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:22:":cpp:logistic_fct_.png";i:1;s:19:"Fonction logistique";i:2;s:6:"center";i:3;s:3:"600";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:2746;}i:139;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2798;}i:140;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:1;i:2;i:2800;}i:2;i:1;i:3;s:3:";#;";}i:2;i:2800;}i:141;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:2803;}i:142;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Figure 1 :";}i:2;i:2805;}i:143;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:2815;}i:144;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:21:"  Fonction logistique";}i:2;i:2817;}i:145;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:3;i:2;i:2817;}i:2;i:3;i:3;s:21:"  Fonction logistique";}i:2;i:2817;}i:146;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:4;i:2;i:2838;}i:2;i:4;i:3;s:4:"
;#;";}i:2;i:2838;}i:147;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2844;}i:148;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:21:"La courbe de Gompertz";i:1;i:4;i:2;i:2844;}i:2;i:2844;}i:149;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:4;}i:2;i:2844;}i:150;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2844;}i:151;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:310:"Elle est utilisée pour modéliser des séries temporelles dont la croissance commence lentement, puis s'arrête à un moment donné. C'est notamment le cas d'une population évoluant dans 
un espace restreint, dont la reproduction augmente au début puis s'arrête lorsque les ressources viennent à manquer. ";}i:2;i:2873;}i:152;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3183;}i:153;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3183;}i:154;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:3185;}i:155;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:3197;}i:156;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3198;}i:157;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:3200;}i:158;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:3210;}i:159;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:154:" Elle est utilisée dans la classification des stades d'évolution de tumeurs, ou de manière générale dans des modèles de prédictions de croissance. ";}i:2;i:3;i:3;s:154:" Elle est utilisée dans la classification des stades d'évolution de tumeurs, ou de manière générale dans des modèles de prédictions de croissance. ";}i:2;i:3212;}i:160;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:3366;}i:161;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3374;}i:162;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3374;}i:163;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:53:"La fonction de Gompertz est donnée par la formule : ";}i:2;i:3376;}i:164;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3429;}i:165;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3429;}i:166;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:2:"$$";i:2;i:1;i:3;s:2:"$$";}i:2;i:3431;}i:167;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:22:"y(t) = He^{-be^{-ct}}$";i:2;i:3;i:3;s:22:"y(t) = He^{-be^{-ct}}$";}i:2;i:3433;}i:168;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:3455;}i:169;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3456;}i:170;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3456;}i:171;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:46:"Avec y(t) la taille d'une tumeur à prédire, ";}i:2;i:3458;}i:172;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:3504;}i:173;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:23:"b = ln(\frac{y_{0}}{H})";i:2;i:3;i:3;s:23:"b = ln(\frac{y_{0}}{H})";}i:2;i:3505;}i:174;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:3528;}i:175;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:", où ";}i:2;i:3529;}i:176;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:3535;}i:177;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:5:"y_{0}";i:2;i:3;i:3;s:5:"y_{0}";}i:2;i:3536;}i:178;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:3541;}i:179;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:115:" est la taille initiale de la tumeur, H la taille maximale qu'elle peut atteindre avec les ressources disponibles, ";}i:2;i:3542;}i:180;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3657;}i:181;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"c";}i:2;i:3659;}i:182;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:3660;}i:183;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:24:" une constante et enfin ";}i:2;i:3662;}i:184;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3686;}i:185;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"t";}i:2;i:3688;}i:186;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:3689;}i:187;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:" le temps.";}i:2;i:3691;}i:188;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3701;}i:189;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3701;}i:190;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:17:":cpp:gompertz.png";i:1;s:18:"Courbe de Gompertz";i:2;s:6:"center";i:3;s:3:"600";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:3703;}i:191;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3749;}i:192;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:1;i:2;i:3751;}i:2;i:1;i:3;s:3:";#;";}i:2;i:3751;}i:193;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3754;}i:194;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Figure 2 :";}i:2;i:3756;}i:195;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:3766;}i:196;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:63:"  Courbe de Gompertz pour l'évolution d'un nombre d'individus";}i:2;i:3768;}i:197;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:3;i:2;i:3768;}i:2;i:3;i:3;s:63:"  Courbe de Gompertz pour l'évolution d'un nombre d'individus";}i:2;i:3768;}i:198;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:4;i:2;i:3831;}i:2;i:4;i:3;s:4:"
;#;";}i:2;i:3831;}i:199;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:3837;}i:200;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:12:"Prédictions";i:1;i:3;i:2;i:3837;}i:2;i:3837;}i:201;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:3837;}i:202;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3837;}i:203;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:155:"Ainsi, dès lors que le modèle a estimé la probabilité p, qu'une observation appartienne à la classe positive, il peut alors effectuer sa prédiction.
";}i:2;i:3859;}i:204;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:2:"\[";i:2;i:1;i:3;s:2:"\[";}i:2;i:4014;}i:205;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:81:" y  =
\begin{cases}
0 & \text{si } p < 0.5\\
1 &\text{si } p \ge 0.5
\end{cases} ";i:2;i:3;i:3;s:81:" y  =
\begin{cases}
0 & \text{si } p < 0.5\\
1 &\text{si } p \ge 0.5
\end{cases} ";}i:2;i:4016;}i:206;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:2:"\]";i:2;i:4;i:3;s:2:"\]";}i:2;i:4097;}i:207;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4099;}i:208;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4099;}i:209;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:233:"Il est nécessaire d'être vigilant, concernant les prédictions proches des frontières de décision. En effet, un léger ajustement du paramétrage peut faire
passer une probabilité de 48 % à 51 %, ce qui altérera la décision. ";}i:2;i:4101;}i:210;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4334;}i:211;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:4336;}i:212;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:33:"Fonction de coût et entrainement";i:1;i:3;i:2;i:4336;}i:2;i:4336;}i:213;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:4336;}i:214;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4336;}i:215;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:99:"L'entrainement de la régression logistique se fait de façon à trouver le vecteur de paramètres ";}i:2;i:4379;}i:216;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:4478;}i:217;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:6:"\theta";i:2;i:3;i:3;s:6:"\theta";}i:2;i:4479;}i:218;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:4485;}i:219;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:164:", qui permet de d'estimer des 
probabilités élevées pour les observations positives et des probabilités basses pour les observations négatives (cf Figure 2).  ";}i:2;i:4486;}i:220;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4650;}i:221;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4650;}i:222;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:66:"Sur l'ensemble du jeu de données, cette fonction, aussi appelée ";}i:2;i:4652;}i:223;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:4718;}i:224;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:16:"perte logistique";}i:2;i:4720;}i:225;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:4736;}i:226;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:75:", est le coût moyen sur l'ensemble des observations, et est donnée par : ";}i:2;i:4738;}i:227;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4813;}i:228;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4813;}i:229;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:2:"$$";i:2;i:1;i:3;s:2:"$$";}i:2;i:4815;}i:230;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:95:"J(\Theta) = - \frac{1}{m}\sum_{i = 1}^{m}[y^{(i)}log(p^{(i)}) + (1 - y^{(i)})log(1 - p^{(i)})]$";i:2;i:3;i:3;s:95:"J(\Theta) = - \frac{1}{m}\sum_{i = 1}^{m}[y^{(i)}log(p^{(i)}) + (1 - y^{(i)})log(1 - p^{(i)})]$";}i:2;i:4817;}i:231;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:4912;}i:232;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:4913;}i:233;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4914;}i:234;a:3:{i:0;s:10:"table_open";i:1;a:3:{i:0;i:2;i:1;i:4;i:2;i:4915;}i:2;i:4914;}i:235;a:3:{i:0;s:15:"tablethead_open";i:1;a:0:{}i:2;i:4914;}i:236;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:4914;}i:237;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:4914;}i:238;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:24:"      Paramètre        ";}i:2;i:4916;}i:239;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:4940;}i:240;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:4940;}i:241;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:" Signification";}i:2;i:4941;}i:242;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:4955;}i:243;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:4956;}i:244;a:3:{i:0;s:16:"tablethead_close";i:1;a:0:{}i:2;i:4956;}i:245;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:4956;}i:246;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:4956;}i:247;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"   ";}i:2;i:4958;}i:248;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:4961;}i:249;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:7:"y^{(i)}";i:2;i:3;i:3;s:7:"y^{(i)}";}i:2;i:4962;}i:250;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:4969;}i:251;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:4970;}i:252;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:4975;}i:253;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:4975;}i:254;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:75:" Probabilité cible que l'observation i appartienne à la classe positive. ";}i:2;i:4976;}i:255;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:5051;}i:256;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:5052;}i:257;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:5052;}i:258;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:5052;}i:259;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"    ";}i:2;i:5054;}i:260;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:5058;}i:261;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"m";i:2;i:3;i:3;s:1:"m";}i:2;i:5059;}i:262;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:5060;}i:263;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:5061;}i:264;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:5066;}i:265;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:5066;}i:266;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:30:" Nombre total d'observations. ";}i:2;i:5067;}i:267;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:5097;}i:268;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:5098;}i:269;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:5098;}i:270;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:5098;}i:271;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"    ";}i:2;i:5100;}i:272;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:5104;}i:273;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:7:"p^{(i)}";i:2;i:3;i:3;s:7:"p^{(i)}";}i:2;i:5105;}i:274;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:5112;}i:275;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:5113;}i:276;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:5118;}i:277;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:5118;}i:278;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:78:" Probabilité estimée que l'observation i appartienne à la classe positive. ";}i:2;i:5119;}i:279;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:5197;}i:280;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:5198;}i:281;a:3:{i:0;s:11:"table_close";i:1;a:1:{i:0;i:5198;}i:2;i:5198;}i:282;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:5198;}i:283;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:5200;}i:284;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:5212;}i:285;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:5213;}i:286;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:5215;}i:287;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:5225;}i:288;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:164:" Il n'existe pas de solution analytique pour résoudre $J(\Theta)$, mais cela reste possible numériquement grâce à une descente de gradient. Vous pouvez cliquer ";}i:2;i:3;i:3;s:164:" Il n'existe pas de solution analytique pour résoudre $J(\Theta)$, mais cela reste possible numériquement grâce à une descente de gradient. Vous pouvez cliquer ";}i:2;i:5227;}i:289;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:21:"regression_supervisee";i:1;s:3:"ici";}i:2;i:5391;}i:290;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:46:" pour plus 
d'informations sur cette méthode.";}i:2;i:3;i:3;s:46:" pour plus 
d'informations sur cette méthode.";}i:2;i:5420;}i:291;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:5466;}i:292;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5474;}i:293;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:5476;}i:294;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:24:"Frontières de décision";i:1;i:3;i:2;i:5476;}i:2;i:5476;}i:295;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:5476;}i:296;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:5476;}i:297;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:5510;}i:298;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:5522;}i:299;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:5523;}i:300;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:9:"Dataset :";}i:2;i:5525;}i:301;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:5534;}i:302;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:92:"  On utilisera le dataset de détection de fraudes, pour un cabinet d'audit, disponible sur ";}i:2;i:3;i:3;s:92:"  On utilisera le dataset de détection de fraudes, pour un cabinet d'audit, disponible sur ";}i:2;i:5536;}i:303;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:111:"https://github.com/LlamasPartan/Machine_Learning_Ressource/tree/master/Classification/Data%20fraude%20bancaires";i:1;s:4:" ici";}i:2;i:5628;}i:304;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:".";}i:2;i:3;i:3;s:1:".";}i:2;i:5748;}i:305;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:5749;}i:306;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5757;}i:307;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:5757;}i:308;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:50:"Le choix du classifieur de mettre une observation ";}i:2;i:5759;}i:309;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:5809;}i:310;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:5:"X_{i}";i:2;i:3;i:3;s:5:"X_{i}";}i:2;i:5810;}i:311;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:5815;}i:312;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:92:" dans une classe précise, se fait à partir de la lecture des probabilités d'appartenance.";}i:2;i:5816;}i:313;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5908;}i:314;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:5908;}i:315;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:25:":cpp:decision_boudary.png";i:1;s:24:"Frontières de décision";i:2;s:6:"center";i:3;s:3:"600";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:5910;}i:316;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5970;}i:317;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:1;i:2;i:5972;}i:2;i:1;i:3;s:3:";#;";}i:2;i:5972;}i:318;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:5975;}i:319;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Figure 3 :";}i:2;i:5977;}i:320;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:5987;}i:321;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:83:"  Frontières de décision pour la détection de fraude et probabilités associées";}i:2;i:5989;}i:322;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:3;i:2;i:5989;}i:2;i:3;i:3;s:83:"  Frontières de décision pour la détection de fraude et probabilités associées";}i:2;i:5989;}i:323;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:4;i:2;i:6072;}i:2;i:4;i:3;s:4:"
;#;";}i:2;i:6072;}i:324;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:6072;}i:325;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:411:"Lorsque l'audit de fraude est donnée par une valeur inférieure ou égale à 1, le classifieur estime avec de fortes probabilités que l'observation décrit une situation normale.
Au contraire, à partir d'une valeur d'audit de risque égale à 1.2, la probabilité d'être dans un cas de fraude est de 60 %. La frontière de décision  se situe donc aux alentours d'une valeur d'audit de risque égale à 1.1.";}i:2;i:6078;}i:326;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:6489;}i:327;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:6491;}i:328;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:7:"Sources";i:1;i:5;i:2;i:6491;}i:2;i:6491;}i:329;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:6491;}i:330;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:6502;}i:331;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:6502;}i:332;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:6502;}i:333;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:6506;}i:334;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:100:"https://fr.wikipedia.org/wiki/Mod%C3%A8le_de_Gompertz#Mod%C3%A9lisation_de_la_croissance_des_tumeurs";i:1;s:10:"Wikipédia";}i:2;i:6507;}i:335;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:6622;}i:336;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:6622;}i:337;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:6622;}i:338;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:6622;}i:339;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:66:" Machine Learning avec Scikit-Learn, 2e édition, Aurélien Géron";}i:2;i:6626;}i:340;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:6692;}i:341;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:6692;}i:342;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:6692;}i:343;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:6692;}i:344;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:75:" Data science : fondamentaux et études de cas, Eric Biernat et Michel Lutz";}i:2;i:6696;}i:345;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:6771;}i:346;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:6771;}i:347;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:6771;}i:348;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:6773;}i:349;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:47:"Modèle de prédiction : Régression logistique";i:1;i:2;i:2;i:6773;}i:2;i:6773;}i:350;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:6773;}i:351;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:6773;}i:352;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:6832;}i:353;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:6844;}i:354;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:6845;}i:355;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:9:"Dataset :";}i:2;i:6847;}i:356;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:6856;}i:357;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:92:"  On utilisera le dataset de détection de fraudes, pour un cabinet d'audit, disponible sur ";}i:2;i:3;i:3;s:92:"  On utilisera le dataset de détection de fraudes, pour un cabinet d'audit, disponible sur ";}i:2;i:6858;}i:358;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:111:"https://github.com/LlamasPartan/Machine_Learning_Ressource/tree/master/Classification/Data%20fraude%20bancaires";i:1;s:4:" ici";}i:2;i:6950;}i:359;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:".";}i:2;i:3;i:3;s:1:".";}i:2;i:7070;}i:360;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:7071;}i:361;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:7079;}i:362;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:7079;}i:363;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:70:"Commençons par créer le classificateur pour la détection de fraude.";}i:2;i:7081;}i:364;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:7151;}i:365;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:7153;}i:366;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:10:"Estimateur";i:1;i:4;i:2;i:7153;}i:2;i:7153;}i:367;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:4;}i:2;i:7153;}i:368;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:7153;}i:369;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:7171;}i:370;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:7173;}i:371;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:7184;}i:372;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:7186;}i:373;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:363:"
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver='liblinear').fit(X_train,y_train)#Entrainement de l'estimateur avec solver='liblinear' car il est plus adapté aux petits datasets
y_pred = model.predict(X_test)#Prédiction sur les données X
y_prob = model.predict_proba(X_test)#Prédiction des probabilités d'appartenance
";i:1;s:6:"python";i:2;N;}i:2;i:7193;}i:374;a:3:{i:0;s:10:"table_open";i:1;a:3:{i:0;i:3;i:1;i:6;i:2;i:7573;}i:2;i:7572;}i:375;a:3:{i:0;s:15:"tablethead_open";i:1;a:0:{}i:2;i:7572;}i:376;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:7572;}i:377;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:7572;}i:378;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"  Solveur  ";}i:2;i:7574;}i:379;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:7585;}i:380;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:7585;}i:381;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:18:"  Fonctionnement  ";}i:2;i:7586;}i:382;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:7604;}i:383;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:7604;}i:384;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:16:" Spécificités ";}i:2;i:7605;}i:385;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:7621;}i:386;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:7622;}i:387;a:3:{i:0;s:16:"tablethead_close";i:1;a:0:{}i:2;i:7622;}i:388;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:7622;}i:389;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:7622;}i:390;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:13:"  newton-cg  ";}i:2;i:7624;}i:391;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:7637;}i:392;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:7637;}i:393;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:145:"  Utilise la matrice hessienne (matrice de dérivées partielles secondes) de la fonction de coût, dans la recherche des paramètres optimaux.  ";}i:2;i:7638;}i:394;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:7783;}i:395;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:7783;}i:396;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:48:" Fonctionne lentement avec les grands datasets. ";}i:2;i:7784;}i:397;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:7832;}i:398;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:7833;}i:399;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:7833;}i:400;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:7833;}i:401;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:9:"  lbfgs  ";}i:2;i:7835;}i:402;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:7844;}i:403;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:7844;}i:404;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:111:"  Approxime la matrice hessienne de la fonction de coût, en évaluant les différents gradients successifs.   ";}i:2;i:7845;}i:405;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:7956;}i:406;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:7956;}i:407;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:60:" Solveur par défaut, il est lent avec les grands datasets. ";}i:2;i:7957;}i:408;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:8017;}i:409;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:8018;}i:410;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:8018;}i:411;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:8018;}i:412;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:13:"  liblinear  ";}i:2;i:8020;}i:413;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:8033;}i:414;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:8033;}i:415;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:78:" Utilise une descente de coordonnées, pour minimiser la fonction de coût.   ";}i:2;i:8034;}i:416;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:8112;}i:417;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:8112;}i:418;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:50:" Fonctionne bien avec de grands jeux de données. ";}i:2;i:8113;}i:419;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:8163;}i:420;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:8164;}i:421;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:8164;}i:422;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:8164;}i:423;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:7:"  sag  ";}i:2;i:8166;}i:424;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:8173;}i:425;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:8173;}i:426;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:57:"  Utilise la descente de gradient stochastique moyenne.  ";}i:2;i:8174;}i:427;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:8231;}i:428;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:8231;}i:429;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:42:" Rapide pour les grands jeux de données. ";}i:2;i:8232;}i:430;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:8274;}i:431;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:8275;}i:432;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:8275;}i:433;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:8275;}i:434;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"  saga  ";}i:2;i:8277;}i:435;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:8285;}i:436;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:8285;}i:437;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:23:"  Extension du solveur ";}i:2;i:8286;}i:438;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:8309;}i:439;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"sag";}i:2;i:8311;}i:440;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:8314;}i:441;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:54:", qui permet l'utilisation de la régularisation L1.  ";}i:2;i:8316;}i:442;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:8370;}i:443;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:8370;}i:444;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:34:" Rapide pour les grands datastes .";}i:2;i:8371;}i:445;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:8405;}i:446;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:8406;}i:447;a:3:{i:0;s:11:"table_close";i:1;a:1:{i:0;i:8406;}i:2;i:8406;}i:448;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:8406;}i:449;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:17:":cpp:solveurs.png";i:1;s:27:"Spécificités des solveurs";i:2;s:6:"center";i:3;s:3:"600";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:8408;}i:450;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:8463;}i:451;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:8463;}i:452;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:8466;}i:453;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:8478;}i:454;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:8479;}i:455;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:8481;}i:456;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:8491;}i:457;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:130:"  Sous R, certains solveurs ne sont pas implémentés. Aussi vous n'aurez la possibilité de n'utiliser que la méthode de Newton.";}i:2;i:3;i:3;s:130:"  Sous R, certains solveurs ne sont pas implémentés. Aussi vous n'aurez la possibilité de n'utiliser que la méthode de Newton.";}i:2;i:8493;}i:458;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:8623;}i:459;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:8631;}i:460;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:8631;}i:461;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:8633;}i:462;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:8635;}i:463;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:8641;}i:464;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:8643;}i:465;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:8643;}i:466;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:8645;}i:467;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:8657;}i:468;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:8658;}i:469;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:8660;}i:470;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:8670;}i:471;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:79:" Les données X de test et d'entrainement doivent être converties en matrices.";}i:2;i:3;i:3;s:79:" Les données X de test et d'entrainement doivent être converties en matrices.";}i:2;i:8672;}i:472;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:8751;}i:473;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:8759;}i:474;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:367:"
library(glmnet)

model <- glmnet(X_train, y_train, family = 'binomial')#Entrainement de l'estimateur, en précisant binomial pour définir la régression logistique binaire
y_pred <- predict(model, newx = X_test, type = 'class')#Prédictions sur les données de test
y_prob <- predict(model, newx = X_test, type = 'response')#Calcul des probabilités d'appartenance
";i:1;s:6:"python";i:2;N;}i:2;i:8766;}i:475;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:9151;}i:476;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:26:"Evaluation de l'estimation";i:1;i:4;i:2;i:9151;}i:2;i:9151;}i:477;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:4;}i:2;i:9151;}i:478;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:9151;}i:479;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:9185;}i:480;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:9187;}i:481;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:9198;}i:482;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:9200;}i:483;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:418:"
from sklearn.metrics import classification_report, confusion_matrix, log_loss

print(confusion_matrix(y_test, y_pred))#Matrice de confusion
print(classification_report(y_test, y_pred))#Résumé des résultats de classification
print('Perte logistique :', log_loss(y_test, y_prob))#Calcul de la perte logistique, métrique importante de la régression logistique. Plus elle est petite, meilleure sont les prédictions
";i:1;s:6:"python";i:2;N;}i:2;i:9207;}i:484;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:9207;}i:485;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:9642;}i:486;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:9644;}i:487;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:9650;}i:488;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:9652;}i:489;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:137:"
library(MLmetrics)

LogLoss(y_pred, y_test)#Calcul de la perte logistique
table(y_test, y_pred)#Construction de la matrice de confusion
";i:1;s:6:"python";i:2;N;}i:2;i:9659;}i:490;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:9659;}i:491;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:9813;}i:492;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:9:"Résultat";}i:2;i:9815;}i:493;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:9824;}i:494;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:9826;}i:495;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:9826;}i:496;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:9828;}i:497;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:9840;}i:498;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:9841;}i:499;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:9843;}i:500;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:9853;}i:501;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:78:"  Pour plus d'informations sur la matrice de confusion, consultez la page sur ";}i:2;i:3;i:3;s:78:"  Pour plus d'informations sur la matrice de confusion, consultez la page sur ";}i:2;i:9855;}i:502;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:36:"evaluer_son_modele_de_classification";i:1;s:40:"évaluation du modèle de classification";}i:2;i:9933;}i:503;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:".";}i:2;i:3;i:3;s:1:".";}i:2;i:10014;}i:504;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:10015;}i:505;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10023;}i:506;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10023;}i:507;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:16:":cpp:report_.png";i:1;s:28:"Résultats de classification";i:2;s:6:"center";i:3;s:3:"500";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:10026;}i:508;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10081;}i:509;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10081;}i:510;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:330:"Il en ressort que sur 102 observations de situations normales, l'estimateur a réussi à toutes les identifier correctement. Pour les 54 observations de cas de fraudes, il 
est parvenu à en identifier correctement 52. Toutefois derrière ces bons résultats, peut se cacher un cas d'over-fitting, qu'il est nécessaire de gérer.";}i:2;i:10083;}i:511;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10413;}i:512;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:10415;}i:513;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:15:"Régularisation";i:1;i:4;i:2;i:10415;}i:2;i:10415;}i:514;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:4;}i:2;i:10415;}i:515;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10415;}i:516;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:10438;}i:517;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:10450;}i:518;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:10451;}i:519;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:10453;}i:520;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:10463;}i:521;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:173:"  Tout comme les modèles de régression linéaire, la régression logistique elle aussi peut être régularisée à l'aide des normes $l_{1}$ et $l_{2}$. Consultez la page ";}i:2;i:3;i:3;s:173:"  Tout comme les modèles de régression linéaire, la régression logistique elle aussi peut être régularisée à l'aide des normes $l_{1}$ et $l_{2}$. Consultez la page ";}i:2;i:10465;}i:522;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:22:"regression_regularisee";i:1;s:54:"régression polynomiale et régressions régularisées";}i:2;i:10638;}i:523;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:50:", pour plus d'informations sur la régularisation.";}i:2;i:3;i:3;s:50:", pour plus d'informations sur la régularisation.";}i:2;i:10719;}i:524;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:10769;}i:525;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10777;}i:526;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10777;}i:527;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:10779;}i:528;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:10781;}i:529;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:10792;}i:530;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10794;}i:531;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:355:"
model = LogisticRegression(solver='liblinear',C=.05, penalty='l2').fit(X_train,y_train)#Entrainement de l'estimateur en définissant un coefficient de régularisation C, et le type de régularisation utilisée
y_pred = model.predict(X_test)#Prédiction sur les données X
y_prob = model.predict_proba(X_test)#Prédiction des probabilités d'appartenance
";i:1;s:6:"python";i:2;N;}i:2;i:10801;}i:532;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10801;}i:533;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:11173;}i:534;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:11175;}i:535;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:11181;}i:536;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:11183;}i:537;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:11183;}i:538;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:11185;}i:539;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:11197;}i:540;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:11198;}i:541;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:11200;}i:542;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:11210;}i:543;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:86:" Il est nécessaire de convertir les données X de test et d'entrainement en matrices.";}i:2;i:3;i:3;s:86:" Il est nécessaire de convertir les données X de test et d'entrainement en matrices.";}i:2;i:11212;}i:544;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:11298;}i:545;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:11306;}i:546;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:452:"
library(glmnet)

model <- glmnet(X_train, y_train, family = 'binomial', alpha = 0, lambda = 0.05)#Entrainement du classificateur avec 
#une régularisation L2 avec alpha = 0, un coefficient de régularisation lambda et family qui définit la régression logistique
y_prob <- predict(model, newx = X_test, type = 'response')#Prédiction des probabilités sur les données de test
y_pred <- ifelse(y_prob > 0.5, 1,0)#Prédiction de l'état de la fraude
";i:1;s:6:"python";i:2;N;}i:2;i:11313;}i:547;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:11313;}i:548;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:11782;}i:549;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Résultat :";}i:2;i:11784;}i:550;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:11795;}i:551;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:11797;}i:552;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:11797;}i:553;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:27:":cpp:regularized_report.png";i:1;s:32:"Résultat après régularisation";i:2;s:6:"center";i:3;s:3:"500";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:11799;}i:554;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:11869;}i:555;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:11869;}i:556;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:179:"L'interprétation de la matrice de confusion est la même que dans le cas précédent. On remarque tout de même une légère perte de précision au niveau de la perte logistique.";}i:2;i:11871;}i:557;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:12050;}i:558;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:12052;}i:559;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:7:"Sources";i:1;i:5;i:2;i:12052;}i:2;i:12052;}i:560;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:12052;}i:561;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:12064;}i:562;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:12064;}i:563;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:12064;}i:564;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:12068;}i:565;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:94:"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html";i:1;s:21:"Documentation Sklearn";}i:2;i:12069;}i:566;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:12189;}i:567;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:12189;}i:568;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:12189;}i:569;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:12189;}i:570;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:12193;}i:571;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:69:"https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/";i:1;s:10:"R-bloggers";}i:2;i:12194;}i:572;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:12278;}i:573;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:12278;}i:574;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:12278;}i:575;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:12278;}i:576;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:12282;}i:577;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:71:"https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451";i:1;s:17:"TowardDataScience";}i:2;i:12283;}i:578;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:12376;}i:579;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:12376;}i:580;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:12376;}i:581;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:12376;}i:582;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:12380;}i:583;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:153:"http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/";i:1;s:5:"STHDA";}i:2;i:12381;}i:584;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:12544;}i:585;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:12544;}i:586;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:12544;}i:587;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:12544;}i:588;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:67:" Machine Learning avec Scikit-Learn, 2e édition,  Aurélien Géron";}i:2;i:12548;}i:589;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:12615;}i:590;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:12615;}i:591;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:12615;}i:592;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:12615;}i:593;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:75:" Data science : fondamentaux et études de cas, Eric Biernat et Michel Lutz";}i:2;i:12619;}i:594;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:12694;}i:595;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:12694;}i:596;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:12694;}i:597;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:12697;}i:598;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:22:"La régression softmax";i:1;i:2;i:2;i:12697;}i:2;i:12697;}i:599;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:12697;}i:600;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:12697;}i:601;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:266:"Il s'agit du cas où le modèle de régression logistique cherche à prédire plus de deux classes non ordonnées. De plus, elle ne doit être utilisée que pour des classes qui ne peuvent survenir en même temps, par exemple plusieurs variétés d'une même plante.";}i:2;i:12731;}i:602;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:12997;}i:603;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:12999;}i:604;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:28:"Estimation des probabilités";i:1;i:3;i:2;i:12999;}i:2;i:12999;}i:605;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:12999;}i:606;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:12999;}i:607;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:136:"Le fonctionnement de la régression softmax repose sur une fonction : la fonction softmax. 
Commençons par considérer une observation ";}i:2;i:13037;}i:608;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:13173;}i:609;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:6:"\bf{x}";i:2;i:3;i:3;s:6:"\bf{x}";}i:2;i:13174;}i:610;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13180;}i:611;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:64:", pour laquelle le modèle softmax va d'abord calculer un score ";}i:2;i:13181;}i:612;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:13245;}i:613;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:13:"S_{k}(\bf{x})";i:2;i:3;i:3;s:13:"S_{k}(\bf{x})";}i:2;i:13246;}i:614;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13259;}i:615;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:23:", pour chaque
classe k.";}i:2;i:13260;}i:616;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:13283;}i:617;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:13283;}i:618;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:2:"$$";i:2;i:1;i:3;s:2:"$$";}i:2;i:13285;}i:619;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:36:"S_{k}(\bf{x}) = (\theta^{(k)})^{T}x$";i:2;i:3;i:3;s:36:"S_{k}(\bf{x}) = (\theta^{(k)})^{T}x$";}i:2;i:13287;}i:620;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13323;}i:621;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:13324;}i:622;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:13325;}i:623;a:3:{i:0;s:10:"table_open";i:1;a:3:{i:0;i:2;i:1;i:4;i:2;i:13326;}i:2;i:13325;}i:624;a:3:{i:0;s:15:"tablethead_open";i:1;a:0:{}i:2;i:13325;}i:625;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:13325;}i:626;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:13325;}i:627;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:24:"      Paramètre        ";}i:2;i:13327;}i:628;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:13351;}i:629;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:13351;}i:630;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:" Signification";}i:2;i:13352;}i:631;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:13366;}i:632;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:13367;}i:633;a:3:{i:0;s:16:"tablethead_close";i:1;a:0:{}i:2;i:13367;}i:634;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:13367;}i:635;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:13367;}i:636;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"   ";}i:2;i:13369;}i:637;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:13372;}i:638;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:6:"\bf{x}";i:2;i:3;i:3;s:6:"\bf{x}";}i:2;i:13373;}i:639;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13379;}i:640;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:13380;}i:641;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:13385;}i:642;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:13385;}i:643;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:29:" Observation d'une variable. ";}i:2;i:13386;}i:644;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:13415;}i:645;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:13416;}i:646;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:13416;}i:647;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:13416;}i:648;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"   ";}i:2;i:13418;}i:649;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:13421;}i:650;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"k";i:2;i:3;i:3;s:1:"k";}i:2;i:13422;}i:651;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13423;}i:652;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:13424;}i:653;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:13429;}i:654;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:13429;}i:655;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:40:" Entier définissant la k-ième classe. ";}i:2;i:13430;}i:656;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:13470;}i:657;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:13471;}i:658;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:13471;}i:659;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:13471;}i:660;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"    ";}i:2;i:13473;}i:661;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:13477;}i:662;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:6:"\Theta";i:2;i:3;i:3;s:6:"\Theta";}i:2;i:13478;}i:663;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13484;}i:664;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:13485;}i:665;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:13490;}i:666;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:13490;}i:667;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:65:" Vecteur de paramètres, regroupant à la fois le terme constant ";}i:2;i:13491;}i:668;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:13556;}i:669;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:10:"\theta_{0}";i:2;i:3;i:3;s:10:"\theta_{0}";}i:2;i:13557;}i:670;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13567;}i:671;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:37:" et les coefficients de pondération ";}i:2;i:13568;}i:672;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:13605;}i:673;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:10:"\theta_{1}";i:2;i:3;i:3;s:10:"\theta_{1}";}i:2;i:13606;}i:674;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13616;}i:675;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:" à ";}i:2;i:13617;}i:676;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:13621;}i:677;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:10:"\theta_{n}";i:2;i:3;i:3;s:10:"\theta_{n}";}i:2;i:13622;}i:678;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13632;}i:679;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:2:". ";}i:2;i:13633;}i:680;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:13635;}i:681;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:13636;}i:682;a:3:{i:0;s:11:"table_close";i:1;a:1:{i:0;i:13636;}i:2;i:13636;}i:683;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:13636;}i:684;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:75:"Une fois que le score de chaque classe a été calculé pour l'observation ";}i:2;i:13638;}i:685;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:13713;}i:686;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:6:"\bf{x}";i:2;i:3;i:3;s:6:"\bf{x}";}i:2;i:13714;}i:687;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13720;}i:688;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:50:", il est alors possible d'estimer la probabilité ";}i:2;i:13721;}i:689;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:13771;}i:690;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:5:"P_{k}";i:2;i:3;i:3;s:5:"P_{k}";}i:2;i:13772;}i:691;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13777;}i:692;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:97:",  que l'observation appartienne à la classe k. 
Cela se fait en transformant les scores par la ";}i:2;i:13778;}i:693;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:13875;}i:694;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:16:"fonction softmax";}i:2;i:13877;}i:695;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:13893;}i:696;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:".";}i:2;i:13895;}i:697;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:13896;}i:698;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:13896;}i:699;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:2:"$$";i:2;i:1;i:3;s:2:"$$";}i:2;i:13898;}i:700;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:94:"P_{k} = \sigma(S(\bf{x}))_{k} = \frac{exp(S_{k}(\bf{x}))}{\sum_{j = 1}^{K}exp(S_{j}(\bf{x}))}$";i:2;i:3;i:3;s:94:"P_{k} = \sigma(S(\bf{x}))_{k} = \frac{exp(S_{k}(\bf{x}))}{\sum_{j = 1}^{K}exp(S_{j}(\bf{x}))}$";}i:2;i:13900;}i:701;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:13994;}i:702;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:13995;}i:703;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:13996;}i:704;a:3:{i:0;s:10:"table_open";i:1;a:3:{i:0;i:2;i:1;i:4;i:2;i:13997;}i:2;i:13996;}i:705;a:3:{i:0;s:15:"tablethead_open";i:1;a:0:{}i:2;i:13996;}i:706;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:13996;}i:707;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:13996;}i:708;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:24:"      Paramètre        ";}i:2;i:13998;}i:709;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:14022;}i:710;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:14022;}i:711;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:" Signification";}i:2;i:14023;}i:712;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:14037;}i:713;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:14038;}i:714;a:3:{i:0;s:16:"tablethead_close";i:1;a:0:{}i:2;i:14038;}i:715;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:14038;}i:716;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:14038;}i:717;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"   ";}i:2;i:14040;}i:718;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:14043;}i:719;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"K";i:2;i:3;i:3;s:1:"K";}i:2;i:14044;}i:720;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:14045;}i:721;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:14046;}i:722;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:14051;}i:723;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:14051;}i:724;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:20:" Nombre de classes. ";}i:2;i:14052;}i:725;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:14072;}i:726;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:14073;}i:727;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:14073;}i:728;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:14073;}i:729;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"   ";}i:2;i:14075;}i:730;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:14078;}i:731;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:9:"S(\bf{x})";i:2;i:3;i:3;s:9:"S(\bf{x})";}i:2;i:14079;}i:732;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:14088;}i:733;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:14089;}i:734;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:14094;}i:735;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:14094;}i:736;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:56:" Vecteur des scores de chaque classe pour l'observation ";}i:2;i:14095;}i:737;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:14151;}i:738;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:6:"\bf{x}";i:2;i:3;i:3;s:6:"\bf{x}";}i:2;i:14152;}i:739;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:14158;}i:740;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:2:". ";}i:2;i:14159;}i:741;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:14161;}i:742;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:14162;}i:743;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:14162;}i:744;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:14162;}i:745;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"    ";}i:2;i:14164;}i:746;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:14168;}i:747;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:21:"\sigma(S(\bf{x}))_{k}";i:2;i:3;i:3;s:21:"\sigma(S(\bf{x}))_{k}";}i:2;i:14169;}i:748;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:14190;}i:749;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:14191;}i:750;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:14196;}i:751;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:14196;}i:752;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:27:" Probabilité estimée que ";}i:2;i:14197;}i:753;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:14224;}i:754;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:6:"\bf{x}";i:2;i:3;i:3;s:6:"\bf{x}";}i:2;i:14225;}i:755;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:14231;}i:756;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:14232;}i:757;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:14233;}i:758;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:3:"\in";i:2;i:3;i:3;s:3:"\in";}i:2;i:14234;}i:759;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:14237;}i:760;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:67:" k, en considérant les scores de chaque classe pour l'observation ";}i:2;i:14238;}i:761;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:14305;}i:762;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:6:"\bf{x}";i:2;i:3;i:3;s:6:"\bf{x}";}i:2;i:14306;}i:763;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:14312;}i:764;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:2:" .";}i:2;i:14313;}i:765;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:14315;}i:766;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:14316;}i:767;a:3:{i:0;s:11:"table_close";i:1;a:1:{i:0;i:14316;}i:2;i:14316;}i:768;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:14318;}i:769;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:12:"Prédictions";i:1;i:3;i:2;i:14318;}i:2;i:14318;}i:770;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:14318;}i:771;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:14318;}i:772;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:124:"Et tout comme la régression logistique, la régression softmax prédit la classe qui a la plus forte probabilité estimée.";}i:2;i:14340;}i:773;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:14464;}i:774;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:14464;}i:775;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:2:"$$";i:2;i:1;i:3;s:2:"$$";}i:2;i:14466;}i:776;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:32:" \text{y = argmax}_{k}^{} P_{k}$";i:2;i:3;i:3;s:32:" \text{y = argmax}_{k}^{} P_{k}$";}i:2;i:14468;}i:777;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:14500;}i:778;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:14501;}i:779;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:14501;}i:780;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:55:"On retourne ainsi la variable qui maximise la fonction ";}i:2;i:14503;}i:781;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:14558;}i:782;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:5:"P_{k}";i:2;i:3;i:3;s:5:"P_{k}";}i:2;i:14559;}i:783;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:14564;}i:784;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:".";}i:2;i:14565;}i:785;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:14566;}i:786;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:14568;}i:787;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:33:"Entrainement et fonction de coût";i:1;i:3;i:2;i:14568;}i:2;i:14568;}i:788;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:14568;}i:789;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:14568;}i:790;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:166:"L'objectif de l'entrainement du modèle softmax, est de d'estimer une probabilité importante pour la classe cible, et donc des probabilités faibles pour les autres.";}i:2;i:14611;}i:791;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:14777;}i:792;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:14777;}i:793;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:57:"Il s'agira donc de minimiser une fonction coût appelée ";}i:2;i:14779;}i:794;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:14836;}i:795;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:17:"entropie croisée";}i:2;i:14838;}i:796;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:14855;}i:797;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:89:", qui pénalise le modèle lorsqu'il estime une probabilité faible pour la classe cible.";}i:2;i:14857;}i:798;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:14946;}i:799;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:14946;}i:800;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:2:"$$";i:2;i:1;i:3;s:2:"$$";}i:2;i:14948;}i:801;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:84:"J(\Theta) = -\frac{1}{m}\sum_{i = 1}^{m}\sum_{k = 1}^{K}y_{k}^{(i)}log(p_{k}^{(i)})$";i:2;i:3;i:3;s:84:"J(\Theta) = -\frac{1}{m}\sum_{i = 1}^{m}\sum_{k = 1}^{K}y_{k}^{(i)}log(p_{k}^{(i)})$";}i:2;i:14950;}i:802;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:15034;}i:803;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:15035;}i:804;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:15035;}i:805;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:15037;}i:806;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:15049;}i:807;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:15050;}i:808;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:15052;}i:809;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:15062;}i:810;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:173:" L'entropie croisée est couramment utilisée pour mesurer la cohérence entre un ensemble de probabilités estimées d'appartenance à des classes et les classes ciblées. ";}i:2;i:3;i:3;s:173:" L'entropie croisée est couramment utilisée pour mesurer la cohérence entre un ensemble de probabilités estimées d'appartenance à des classes et les classes ciblées. ";}i:2;i:15064;}i:811;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:15237;}i:812;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:15245;}i:813;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:15246;}i:814;a:3:{i:0;s:10:"table_open";i:1;a:3:{i:0;i:2;i:1;i:5;i:2;i:15247;}i:2;i:15246;}i:815;a:3:{i:0;s:15:"tablethead_open";i:1;a:0:{}i:2;i:15246;}i:816;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:15246;}i:817;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:15246;}i:818;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:24:"      Paramètre        ";}i:2;i:15248;}i:819;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:15272;}i:820;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:15272;}i:821;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:" Signification";}i:2;i:15273;}i:822;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:15287;}i:823;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:15288;}i:824;a:3:{i:0;s:16:"tablethead_close";i:1;a:0:{}i:2;i:15288;}i:825;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:15288;}i:826;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:15288;}i:827;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"   ";}i:2;i:15290;}i:828;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:15293;}i:829;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:11:"y_{k}^{(i)}";i:2;i:3;i:3;s:11:"y_{k}^{(i)}";}i:2;i:15294;}i:830;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:15305;}i:831;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:15306;}i:832;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:15311;}i:833;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:15311;}i:834;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:64:" Probabilité cible que la classe i appartienne à la classe k. ";}i:2;i:15312;}i:835;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:15376;}i:836;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:15377;}i:837;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:15377;}i:838;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:15377;}i:839;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:3:"   ";}i:2;i:15379;}i:840;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:15382;}i:841;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"K";i:2;i:3;i:3;s:1:"K";}i:2;i:15383;}i:842;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:15384;}i:843;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:15385;}i:844;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:15390;}i:845;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:15390;}i:846;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:26:" Nombre total de classes. ";}i:2;i:15391;}i:847;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:15417;}i:848;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:15418;}i:849;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:15418;}i:850;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:15418;}i:851;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"    ";}i:2;i:15420;}i:852;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:15424;}i:853;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:11:"p_{k}^{(i)}";i:2;i:3;i:3;s:11:"p_{k}^{(i)}";}i:2;i:15425;}i:854;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:15436;}i:855;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:15437;}i:856;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:15442;}i:857;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:15442;}i:858;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:80:" Probabilité estimée que la classe i appartienne effectivement à la classe k.";}i:2;i:15443;}i:859;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:15523;}i:860;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:15524;}i:861;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:15524;}i:862;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:15524;}i:863;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:"    ";}i:2;i:15526;}i:864;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:1;i:3;s:1:"$";}i:2;i:15530;}i:865;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"m";i:2;i:3;i:3;s:1:"m";}i:2;i:15531;}i:866;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:18:"mathjax_protecttex";i:1;s:1:"$";i:2;i:4;i:3;s:1:"$";}i:2;i:15532;}i:867;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:5:"     ";}i:2;i:15533;}i:868;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:15538;}i:869;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:15538;}i:870;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:30:" Nombre total d'observations. ";}i:2;i:15539;}i:871;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:15569;}i:872;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:15570;}i:873;a:3:{i:0;s:11:"table_close";i:1;a:1:{i:0;i:15570;}i:2;i:15570;}i:874;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:15572;}i:875;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:6:"Source";i:1;i:5;i:2;i:15572;}i:2;i:15572;}i:876;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:15572;}i:877;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:15583;}i:878;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:15583;}i:879;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:15583;}i:880;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:66:" Machine Learning avec Scikit-Learn, 2e édition, Aurélien Géron";}i:2;i:15587;}i:881;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:15653;}i:882;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:15653;}i:883;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:15653;}i:884;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:15656;}i:885;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:44:"Modèle de prédiction : Régression softmax";i:1;i:2;i:2;i:15656;}i:2;i:15656;}i:886;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:15656;}i:887;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:15656;}i:888;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:78:"Par défaut, la classe LogisticRegression de Scikit-Learn utilise la méthode ";}i:2;i:15712;}i:889;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:15790;}i:890;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:15:"one-vs-the-rest";}i:2;i:15792;}i:891;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:15807;}i:892;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:158:" lors de la prédiction sur plus de deux classes.
Néanmoins, pour la transformer en régression softmax, il est nécessaire d'initialiser l'hyper-paramètre ";}i:2;i:15809;}i:893;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:15967;}i:894;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"multi_class";}i:2;i:15969;}i:895;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:15980;}i:896;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:2:". ";}i:2;i:15982;}i:897;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:15984;}i:898;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:15984;}i:899;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:15986;}i:900;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:15998;}i:901;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:15999;}i:902;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:9:"Dataset :";}i:2;i:16001;}i:903;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:16010;}i:904;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:61:"  On utilisera le dataset de qualité de vin, disponible sur ";}i:2;i:3;i:3;s:61:"  On utilisera le dataset de qualité de vin, disponible sur ";}i:2;i:16012;}i:905;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:96:"https://github.com/LlamasPartan/Machine_Learning_Ressource/tree/master/Classification/Data%20vin";i:1;s:4:" ici";}i:2;i:16073;}i:906;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:".";}i:2;i:3;i:3;s:1:".";}i:2;i:16178;}i:907;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:16179;}i:908;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:16187;}i:909;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:16187;}i:910;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:16189;}i:911;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:16191;}i:912;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:16202;}i:913;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:16204;}i:914;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:312:"
model = LogisticRegression(solver = 'newton-cg',multi_class = "multinomial", penalty = 'l2', C = 0.6000000000000001).fit(X_train,y_train)#Entrainement de l'estimateur
y_pred = model.predict(X_test)#Prédiction sur les données X
y_prob = model.predict_proba(X_test)#Prédiction des probabilités d'appartenance
";i:1;s:6:"python";i:2;N;}i:2;i:16211;}i:915;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:16211;}i:916;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:16540;}i:917;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:16542;}i:918;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:16548;}i:919;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:16550;}i:920;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:340:"
model <- glmnet(X_train, y_train, family = 'multinomial',  type.logistic = "Newton", alpha = 0, lambda = 0.6000000000000001)#Entrainement de l'estimateur
y_pred <- predict(model, newx = X_test, type = 'class')#Prédiction des classes 
y_prob <- predict(model, newx = X_test, type = 'response')#Prédiction des probabilités d'appartenance
";i:1;s:6:"python";i:2;N;}i:2;i:16557;}i:921;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:16557;}i:922;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:25:":cpp:multinomial_data.png";i:1;s:40:"Résultat de classification multinomiale";i:2;s:6:"center";i:3;s:3:"500";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:16914;}i:923;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:16990;}i:924;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:16990;}i:925;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:48:"Il en ressort que pour 7 observations de classe ";}i:2;i:16992;}i:926;a:3:{i:0;s:18:"doublequoteopening";i:1;a:0:{}i:2;i:17040;}i:927;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"1";}i:2;i:17041;}i:928;a:3:{i:0;s:18:"doublequoteclosing";i:1;a:0:{}i:2;i:17042;}i:929;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:108:", le classifieur a réussi à toutes les déterminer. Il en est de même pour les 17 observations de classe ";}i:2;i:17043;}i:930;a:3:{i:0;s:18:"doublequoteopening";i:1;a:0:{}i:2;i:17151;}i:931;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"2";}i:2;i:17152;}i:932;a:3:{i:0;s:18:"doublequoteclosing";i:1;a:0:{}i:2;i:17153;}i:933;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:43:".
Enfin pour les 12 observations de classe ";}i:2;i:17154;}i:934;a:3:{i:0;s:18:"doublequoteopening";i:1;a:0:{}i:2;i:17197;}i:935;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"3";}i:2;i:17198;}i:936;a:3:{i:0;s:18:"doublequoteclosing";i:1;a:0:{}i:2;i:17199;}i:937;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:98:", le modèle a réussi à en déterminer 11, en classant une observation appartenant à la classe ";}i:2;i:17200;}i:938;a:3:{i:0;s:18:"doublequoteopening";i:1;a:0:{}i:2;i:17298;}i:939;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"2";}i:2;i:17299;}i:940;a:3:{i:0;s:18:"doublequoteclosing";i:1;a:0:{}i:2;i:17300;}i:941;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:".";}i:2;i:17301;}i:942;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:17302;}i:943;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:17305;}i:944;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:31:"Régression logistique ordinale";i:1;i:2;i:2;i:17305;}i:2;i:17305;}i:945;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:17305;}i:946;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:17305;}i:947;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:135:"Ce type de régression logistique s'applique lorsque la variable cible contient plus de deux classes, liées par une relation d'ordre. ";}i:2;i:17348;}i:948;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:17483;}i:949;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:17483;}i:950;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:17485;}i:951;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:17497;}i:952;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:17498;}i:953;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:9:"Dataset :";}i:2;i:17500;}i:954;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:17509;}i:955;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:59:"  On reprend le dataset de qualité de vin, disponible sur ";}i:2;i:3;i:3;s:59:"  On reprend le dataset de qualité de vin, disponible sur ";}i:2;i:17511;}i:956;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:96:"https://github.com/LlamasPartan/Machine_Learning_Ressource/tree/master/Classification/Data%20vin";i:1;s:4:" ici";}i:2;i:17570;}i:957;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:122:". La méthode restera 
la même pour des données ayant une vraie relation d'ordre entre les classes de la variable cible.";}i:2;i:3;i:3;s:122:". La méthode restera 
la même pour des données ayant une vraie relation d'ordre entre les classes de la variable cible.";}i:2;i:17675;}i:958;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:17797;}i:959;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:17805;}i:960;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:17805;}i:961;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:17807;}i:962;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:17809;}i:963;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:17820;}i:964;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:17822;}i:965;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:304:"
import mord as md

model = md.LogisticAT(alpha=0).fit(X_train, y_train)#Entrainement du modèle, en précisant alpha=0 pour ne pas inclure de régularisation
y_pred = model.predict(X_test)#Prédiction sur les données X
y_prob = model.predict_proba(X_test)#Prédiction des probabilités d'appartenance 
";i:1;s:6:"python";i:2;N;}i:2;i:17829;}i:966;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:17829;}i:967;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:18150;}i:968;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:18152;}i:969;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:18158;}i:970;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18160;}i:971;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18160;}i:972;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:131:"Il est nécessaire à cette étape d'avoir choisi les variables les plus importantes pour le modèle, afin de le faire fonctionner.";}i:2;i:18162;}i:973;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18293;}i:974;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:301:"
library(MASS)

model <- polr(as.factor(Wine)~ Alcohol + Malic.acid + Ash + Acl + Mg, data = data_test, method = 'logistic')#Entrainement du modèle
y_pred <- predict(model, X_test)#Prédiction sur les données X
y_prob <- predict(model, X_test, type='p')#Prédiction des probabilités d'appartenance
";i:1;s:6:"python";i:2;N;}i:2;i:18300;}i:975;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18300;}i:976;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:52:"Maintenant on affiche les résultats de prédiction.";}i:2;i:18618;}i:977;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18670;}i:978;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18670;}i:979;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:18672;}i:980;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:18674;}i:981;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:18685;}i:982;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18687;}i:983;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:339:"
print(confusion_matrix(y_test, y_pred))#Matrice de confusion
print(classification_report(y_test, y_pred))#Résumé des résultats de classification
print('Perte logistique :', log_loss(y_test, y_prob))#Calcul de la perte logistique, métrique importante de la régression logistique. Plus elle est petite, meilleure sont les prédictions
";i:1;s:6:"python";i:2;N;}i:2;i:18694;}i:984;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18694;}i:985;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:19050;}i:986;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:19052;}i:987;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:19058;}i:988;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:19060;}i:989;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:19060;}i:990;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:19062;}i:991;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:9:"Résultat";}i:2;i:19064;}i:992;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:19073;}i:993;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:19075;}i:994;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:19075;}i:995;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:25:":cpp:resultat_ordinal.png";i:1;s:24:"Résultat de prédiction";i:2;s:6:"center";i:3;s:3:"500";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:19077;}i:996;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:19137;}i:997;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:19137;}i:998;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:145:"Notre classifieur prédit globalement très bien le résultat de chaque classe, et cela peut se confirmer avec une perte logistique très faible.";}i:2;i:19139;}i:999;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:19284;}i:1000;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:19286;}i:1001;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:6:"Source";i:1;i:5;i:2;i:19286;}i:2;i:19286;}i:1002;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:19286;}i:1003;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:19296;}i:1004;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:19296;}i:1005;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:19296;}i:1006;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:19300;}i:1007;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:68:"https://pythonhosted.org/mord/reference.html#mord.MulticlassLogistic";i:1;s:18:"Mord Documentation";}i:2;i:19301;}i:1008;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:19392;}i:1009;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:19392;}i:1010;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:19392;}i:1011;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:19392;}i:1012;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:19396;}i:1013;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:205:"http://r-statistics.co/Ordinal-Logistic-Regression-With-R.html#:~:text=r%2Dstatistics.co%20by%20Selva%20Prabhakaran&text=Ordinal%20logistic%20regression%20can%20be,of%20multi%2Dclass%20ordered%20variables.";i:1;s:11:"R-Statistic";}i:2;i:19397;}i:1014;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:19618;}i:1015;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:19618;}i:1016;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:19618;}i:1017;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:19620;}i:1018;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:51:"Régression logistique : Avantages & inconvénients";i:1;i:2;i:2;i:19620;}i:2;i:19620;}i:1019;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:19620;}i:1020;a:3:{i:0;s:10:"table_open";i:1;a:3:{i:0;i:2;i:1;i:3;i:2;i:19683;}i:2;i:19682;}i:1021;a:3:{i:0;s:15:"tablethead_open";i:1;a:0:{}i:2;i:19682;}i:1022;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:19682;}i:1023;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:19682;}i:1024;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:23:"      Avantages        ";}i:2;i:19684;}i:1025;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:19707;}i:1026;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:19707;}i:1027;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:16:" Inconvénients ";}i:2;i:19708;}i:1028;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:19724;}i:1029;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:19725;}i:1030;a:3:{i:0;s:16:"tablethead_close";i:1;a:0:{}i:2;i:19725;}i:1031;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:19725;}i:1032;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:19725;}i:1033;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:44:"   Explicabilité des résultats obtenus.   ";}i:2;i:19727;}i:1034;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:19771;}i:1035;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:19771;}i:1036;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:90:" L'hypothèse de linéarité du score, compromet les relations complexes entre variables. ";}i:2;i:19772;}i:1037;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:19862;}i:1038;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:19863;}i:1039;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:19863;}i:1040;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:6:"center";i:2;i:1;}i:2;i:19863;}i:1041;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:111:"   Modèle peu susceptible d'être en situation d'over-fitting, du fait de la simplicité de l'algorithme.     ";}i:2;i:19865;}i:1042;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:19976;}i:1043;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:19976;}i:1044;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:146:" Très bonnes performances pour une classification binaire, mais plus il y a de classes à prédire, moins bonne sera la qualité de l'algorithme.";}i:2;i:19977;}i:1045;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:20123;}i:1046;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:20124;}i:1047;a:3:{i:0;s:11:"table_close";i:1;a:1:{i:0;i:20124;}i:2;i:20124;}i:1048;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:20127;}i:1049;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:6:"Source";i:1;i:5;i:2;i:20127;}i:2;i:20127;}i:1050;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:20127;}i:1051;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:20137;}i:1052;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:20137;}i:1053;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:20137;}i:1054;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:104:" Big Data et Machine Learning, Manuel du data scientist, P. Lemberger, M. Batty, M. Morel, JL. Raffaelli";}i:2;i:20141;}i:1055;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:20245;}i:1056;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:20245;}i:1057;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:20245;}i:1058;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:20245;}i:1059;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:20245;}}