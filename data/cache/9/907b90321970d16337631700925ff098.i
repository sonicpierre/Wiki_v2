a:719:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:0;}i:2;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:6:"cpp:IA";i:1;s:17:" Machine Learning";}i:2;i:1;}i:3;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:29;}i:4;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:18:":cpp:dataexplo.jpg";i:1;s:0:"";i:2;s:6:"center";i:3;s:3:"500";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:30;}i:5;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:59;}i:6;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:59;}i:7;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:245:"Cette section va aborder les différentes façons d'aborder un problème de Machine Learning et d'exploiter les données associées. On parlera aussi du nettoyage de données, actions qui représente environ 80 % du travail de data scientist. ";}i:2;i:61;}i:8;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:306;}i:9;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:13:"Le clustering";i:1;i:2;i:2;i:308;}i:2;i:308;}i:10;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:308;}i:11;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:308;}i:12;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:242:"Il s'agit ici d'apprentissage non supervisé. La machine va apprendre toute seule, à reconnaître les différents groupements de données. Il y a plusieurs algorithmes pour cela, mais parmi les plus connus on a les algorithmes KMeans et CAH.";}i:2;i:333;}i:13;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:575;}i:14;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:575;}i:15;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:577;}i:16;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:589;}i:17;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:590;}i:18;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:592;}i:19;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:602;}i:20;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:97:" Dans cette partie, on utilisera le dataset de mesures de gaz contenus dans l'alcool, disponible ";}i:2;i:3;i:3;s:97:" Dans cette partie, on utilisera le dataset de mesures de gaz contenus dans l'alcool, disponible ";}i:2;i:604;}i:21;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:109:"https://github.com/LlamasPartan/Machine_Learning_Ressource/tree/master/Exploration%20des%20donnees/Data%20gaz";i:1;s:3:"ici";}i:2;i:701;}i:22;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:37:". On considérera le premier relevé.";}i:2;i:3;i:3;s:37:". On considérera le premier relevé.";}i:2;i:818;}i:23;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:855;}i:24;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:863;}i:25;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:865;}i:26;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:11:"Les K-means";i:1;i:3;i:2;i:865;}i:2;i:865;}i:27;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:865;}i:28;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:865;}i:29;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:16:":cpp:k_means.png";i:1;s:31:"Segmentation du jeu de données";i:2;s:6:"center";i:3;s:4:"1000";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:886;}i:30;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:945;}i:31;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:945;}i:32;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:249:"Le partitionnement s'opère par le fait que l'ordinateur cherche à trouver le centre de gravité de chaque paquet (cluster). A chaque itération, il affecte les points au cluster le plus proche puis recommence le processus jusqu'à un état stable.";}i:2;i:947;}i:33;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1196;}i:34;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1196;}i:35;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:103:"Il est parfois nécessaire d'adapter la distance pour calculer l'éloignement aux centres de gravité. ";}i:2;i:1198;}i:36;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1301;}i:37;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1303;}i:38;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:26:"Normalisation des données";i:1;i:5;i:2;i:1303;}i:2;i:1303;}i:39;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:1303;}i:40;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1303;}i:41;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:1335;}i:42;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:1347;}i:43;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:1348;}i:44;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:1350;}i:45;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:1360;}i:46;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:109:" Il est judicieux de centrer les données avant d'appliquer les algorithmes de clustering. Consultez la page ";}i:2;i:3;i:3;s:109:" Il est judicieux de centrer les données avant d'appliquer les algorithmes de clustering. Consultez la page ";}i:2;i:1362;}i:47;a:3:{i:0;s:12:"internallink";i:1;a:2:{i:0;s:25:"preprocessing_et_encodage";i:1;s:26:"pre-processing et encodage";}i:2;i:1471;}i:48;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:26:" pour plus d'informations.";}i:2;i:3;i:3;s:26:" pour plus d'informations.";}i:2;i:1527;}i:49;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:1553;}i:50;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1561;}i:51;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1561;}i:52;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:1564;}i:53;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:1566;}i:54;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:1577;}i:55;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1579;}i:56;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:273:"
from sklearn.preprocessing import StandardScaler
 
scaler = StandardScaler()#Appel du transformeur
scaler.fit_transform(data)#Transformation des données
data = pd.DataFrame(scaler.fit_transform(data), index = data.index, columns = data.columns)#Construction du DataFrame
";i:1;s:6:"python";i:2;N;}i:2;i:1586;}i:57;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1586;}i:58;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:1876;}i:59;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:1878;}i:60;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:1884;}i:61;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1886;}i:62;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:21:"
data <- scale(data)
";i:1;s:6:"python";i:2;N;}i:2;i:1893;}i:63;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1931;}i:64;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:24:"Le modèle de clustering";i:1;i:5;i:2;i:1931;}i:2;i:1931;}i:65;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:1931;}i:66;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:1931;}i:67;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:42:"Construction du modèle de partitionnement";}i:2;i:1961;}i:68;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2003;}i:69;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2003;}i:70;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:2005;}i:71;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:2007;}i:72;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:2018;}i:73;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2020;}i:74;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:286:"
from sklearn.cluster import KMeans
 
model = KMeans(n_clusters = 3, max_iter=500, init='k-means++')#Création du modèle, initialisation du nombre de clusters et d'itérations
#Définition de la méthode d'initialisation des centres de gravité
model.fit(data)#Entrainement du modèle
";i:1;s:6:"python";i:2;N;}i:2;i:2027;}i:75;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2027;}i:76;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:2330;}i:77;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:2332;}i:78;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:2338;}i:79;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2340;}i:80;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:113:"
library(cluster)

model = kmeans(data, center=3)#data étant le jeu de données et center le nombre de clusters
";i:1;s:6:"python";i:2;N;}i:2;i:2347;}i:81;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:2477;}i:82;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:30:"Visualisation des prédictions";i:1;i:5;i:2;i:2477;}i:2;i:2477;}i:83;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:2477;}i:84;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2477;}i:85;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:50:"On peut vérifier les résultats de prédiction : ";}i:2;i:2513;}i:86;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2563;}i:87;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2563;}i:88;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:2565;}i:89;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:2567;}i:90;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:2578;}i:91;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2580;}i:92;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:305:"
plt.figure(figsize=(12, 6))#Définition de la taille du graphique

plt.scatter(data['0.799_0.201'], data['0.799_0.201.1'], c=model.predict(data))#Nuage de points des clusters 
plt.scatter(model.cluster_centers_[:,0], model.cluster_centers_[:,1], c='r')#Positionnement des centres des différent clusters
";i:1;s:6:"python";i:2;N;}i:2;i:2587;}i:93;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:2587;}i:94;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:2909;}i:95;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:2911;}i:96;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:2917;}i:97;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:2919;}i:98;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:163:"
plot(data, col = model$cluster)#Nuage de points de points des clusters
points(model$centers, col = 1:2, pch = 8, cex = 2)#Positionnement des centres des clusters
";i:1;s:6:"python";i:2;N;}i:2;i:2926;}i:99;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:3106;}i:100;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:9:"Résultat";i:1;i:5;i:2;i:3106;}i:2;i:3106;}i:101;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:3106;}i:102;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3106;}i:103;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:25:":cpp:clustering_clus3.png";i:1;s:39:"Résultat du clustering avec 3 clusters";i:2;s:6:"center";i:3;s:3:"600";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:3121;}i:104;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3196;}i:105;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:1;i:2;i:3198;}i:2;i:1;i:3;s:3:";#;";}i:2;i:3198;}i:106;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3201;}i:107;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Figure 1 :";}i:2;i:3203;}i:108;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:3213;}i:109;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:40:" Résultat du clustering avec 3 clusters";}i:2;i:3215;}i:110;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:3;i:2;i:3215;}i:2;i:3;i:3;s:40:" Résultat du clustering avec 3 clusters";}i:2;i:3215;}i:111;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:4;i:2;i:3255;}i:2;i:4;i:3;s:4:"
;#;";}i:2;i:3255;}i:112;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3255;}i:113;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:265:"On remarque que, parmi les trois clusters défini initialement, seul l'un d'entre eux correspond à son centre de gravité. Cela veut donc dire que le nombre de clusters défini n'est pas exact, vu qu'il n'y a pas de convergence des deux autres centres de gravité.";}i:2;i:3261;}i:114;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3526;}i:115;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3526;}i:116;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:46:"De ce fait on utilisera une méthode appelée ";}i:2;i:3528;}i:117;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:3574;}i:118;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:12:"Elbow Method";}i:2;i:3576;}i:119;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:3588;}i:120;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:197:",  qui calcule la variance expliquée par chacun des clusters. On verra ainsi la courbe de variance expliquée et on déterminera le nombre de clusters optimaux, expliquant au maximum nos données.";}i:2;i:3590;}i:121;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3787;}i:122;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3787;}i:123;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:3789;}i:124;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:3791;}i:125;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:3802;}i:126;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:3804;}i:127;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:543:"

#Partie entrainement
 
inertia = []#Initialisation d'un tableau d'inertie
K_range = range(1,20)#Création d'une liste de nombre de cluster
for k in K_range:
    model = KMeans(n_clusters = k).fit(data)#Entrainement du modèle pour chaque nombre de clusters de la liste
    inertia.append(model.inertia_)#Ajout de l'inertie de chaque modèle
 
#Partie visualisation
 
plt.plot(K_range, inertia)#Graphique d'inertie de chaque modèle en fonction du nombre de clusters associés
plt.xlabel('Nombre de cluster')
plt.ylabel('Inertie expliquée')
";i:1;s:6:"python";i:2;N;}i:2;i:3811;}i:128;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:3811;}i:129;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:4371;}i:130;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:4373;}i:131;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:4379;}i:132;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4381;}i:133;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:176:"
R2 = vector ("numeric", 9)
for(k in 2:10)           #pour i allant de 2 à
{
  cl = kmeans(data, centers=k, nstart=5)
  R2[k-1]=cl$betweenss/cl$totss
}
plot(2:10,R2,type="b")
";i:1;s:6:"python";i:2;N;}i:2;i:4388;}i:134;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:4581;}i:135;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:9:"Résultat";i:1;i:5;i:2;i:4581;}i:2;i:4581;}i:136;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:4581;}i:137;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4581;}i:138;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:14:":cpp:elbow.png";i:1;s:54:"Inertie des modèles en fonction du nombre de clusters";i:2;s:6:"center";i:3;s:3:"600";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:4596;}i:139;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4675;}i:140;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:1;i:2;i:4677;}i:2;i:1;i:3;s:3:";#;";}i:2;i:4677;}i:141;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:4680;}i:142;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Figure 2 :";}i:2;i:4682;}i:143;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:4692;}i:144;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:55:" Inertie des modèles en fonction du nombre de clusters";}i:2;i:4694;}i:145;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:3;i:2;i:4694;}i:2;i:3;i:3;s:55:" Inertie des modèles en fonction du nombre de clusters";}i:2;i:4694;}i:146;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:4;i:2;i:4749;}i:2;i:4;i:3;s:4:"
;#;";}i:2;i:4749;}i:147;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4749;}i:148;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:200:"Il en ressort que le jeu de données utilisé contient cinq clusters, qu'on peut mieux visualiser après modification du paramètre. On remarque aussi que les centres de gravité sont mieux répartis.";}i:2;i:4755;}i:149;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:4955;}i:150;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:4955;}i:151;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:30:":cpp:lustering_after_elbow.png";i:1;s:45:"Répartition des clusters après optimisation";i:2;s:6:"center";i:3;s:3:"600";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:4957;}i:152;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5043;}i:153;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:1;i:2;i:5045;}i:2;i:1;i:3;s:3:";#;";}i:2;i:5045;}i:154;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:5048;}i:155;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Figure 3 :";}i:2;i:5050;}i:156;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:5060;}i:157;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:37:" Clusters optimaux du jeu de données";}i:2;i:5062;}i:158;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:3;i:2;i:5062;}i:2;i:3;i:3;s:37:" Clusters optimaux du jeu de données";}i:2;i:5062;}i:159;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:4;i:2;i:5099;}i:2;i:4;i:3;s:4:"
;#;";}i:2;i:5099;}i:160;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:5106;}i:161;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:7:"Sources";i:1;i:5;i:2;i:5106;}i:2;i:5106;}i:162;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:5106;}i:163;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:5118;}i:164;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:5118;}i:165;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:5118;}i:166;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:5122;}i:167;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:92:"https://www.youtube.com/watch?v=FTtzd31IAOw&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=28";i:1;s:44:" Machine Learnia, par Guillaume Saint-Cirgue";}i:2;i:5123;}i:168;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:5264;}i:169;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:5264;}i:170;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:5264;}i:171;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:5264;}i:172;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:63:" Cours de Data Mining : Clustering, par Astrid Jourdan, CY Tech";}i:2;i:5268;}i:173;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:5331;}i:174;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:5331;}i:175;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:5331;}i:176;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:5332;}i:177;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:42:"La Classification Ascendante Hiérarchique";i:1;i:3;i:2;i:5332;}i:2;i:5332;}i:178;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:5332;}i:179;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:5332;}i:180;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:13:":cpp:cah_.png";i:1;s:3:"CAH";i:2;s:6:"center";i:3;s:3:"450";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:5384;}i:181;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5411;}i:182;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:5411;}i:183;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:288:"Au début de l'algorithme chaque individu forme une classe. Puis, à chaque itération on regroupe les individus les plus proches et on regarde la perte d'information sous la forme d'un dendrogramme. A partir de ce graphique on choisit finalement combien de clusters on décide de garder.";}i:2;i:5413;}i:184;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5701;}i:185;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:5701;}i:186;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:5703;}i:187;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:5705;}i:188;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:5716;}i:189;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5718;}i:190;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:132:"
from scipy.cluster import hierarchy

Z = hierarchy.linkage(data, method='ward')#Définition de la méthode de calcul des distances
";i:1;s:6:"python";i:2;N;}i:2;i:5725;}i:191;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:5725;}i:192;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:5874;}i:193;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:5876;}i:194;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:5882;}i:195;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:5884;}i:196;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:86:"
distance = dist(x, "euclidean") #crée une structure de distance entre les individus
";i:1;s:6:"python";i:2;N;}i:2;i:5891;}i:197;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:5891;}i:198;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:92:"On peut ensuite dessiner le dendrogramme pour mieux visualiser les clusters qui ressortent :";}i:2;i:5994;}i:199;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:6086;}i:200;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:6086;}i:201;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:6088;}i:202;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:6090;}i:203;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:6101;}i:204;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:6103;}i:205;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:220:"
plt.figure(figsize=(12,6))#Définition de la taille du graphique

dendrogramme = hierarchy.dendrogram(Z)#Contruction du dendrogramme selon les paramètres fournis

plt.xlabel('Taille du cluster')
plt.ylabel('Distance')
";i:1;s:6:"python";i:2;N;}i:2;i:6111;}i:206;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:6111;}i:207;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:6348;}i:208;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:6350;}i:209;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:6356;}i:210;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:6358;}i:211;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:162:"
h = hclust(distance, "ward.D2")#Création des paramètres du dendroramme
c = cutree(h, k=5)#Création des différentes classes
plot(h)#Création du dendrogramme
";i:1;s:6:"python";i:2;N;}i:2;i:6365;}i:212;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:6544;}i:213;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:9:"Résultat";i:1;i:5;i:2;i:6544;}i:2;i:6544;}i:214;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:6544;}i:215;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:6544;}i:216;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:21:":cpp:dendrogramme.png";i:1;s:12:"Dendrogramme";i:2;s:6:"center";i:3;s:3:"600";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:6559;}i:217;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:6603;}i:218;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:1;i:2;i:6605;}i:2;i:1;i:3;s:3:";#;";}i:2;i:6605;}i:219;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:6608;}i:220;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Figure 4 :";}i:2;i:6610;}i:221;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:6620;}i:222;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:32:" Dendrogramme du jeu de données";}i:2;i:6622;}i:223;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:3;i:2;i:6622;}i:2;i:3;i:3;s:32:" Dendrogramme du jeu de données";}i:2;i:6622;}i:224;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:4;i:2;i:6654;}i:2;i:4;i:3;s:4:"
;#;";}i:2;i:6654;}i:225;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:6661;}i:226;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:6:"Source";i:1;i:5;i:2;i:6661;}i:2;i:6661;}i:227;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:6661;}i:228;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:6672;}i:229;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:6672;}i:230;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:6672;}i:231;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:6676;}i:232;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:43:"https://www.youtube.com/watch?v=JcfIeaGzF8A";i:1;s:19:"TheEngineeringWorld";}i:2;i:6677;}i:233;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:6744;}i:234;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:6744;}i:235;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:6744;}i:236;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:6745;}i:237;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:37:"L'Analyse des Composantes Principales";i:1;i:2;i:2;i:6745;}i:2;i:6745;}i:238;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:6745;}i:239;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:6745;}i:240;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:6794;}i:241;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:6806;}i:242;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:6807;}i:243;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:6809;}i:244;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:6819;}i:245;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:90:" On utilisera de nouveau, le dataset de mesures de gaz contenus dans l'alcool, disponible ";}i:2;i:3;i:3;s:90:" On utilisera de nouveau, le dataset de mesures de gaz contenus dans l'alcool, disponible ";}i:2;i:6821;}i:246;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:66:"https://archive.ics.uci.edu/ml/datasets/Alcohol+QCM+Sensor+Dataset";i:1;s:3:"ici";}i:2;i:6911;}i:247;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:37:". On considérera le premier relevé.";}i:2;i:3;i:3;s:37:". On considérera le premier relevé.";}i:2;i:6985;}i:248;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:7022;}i:249;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:7030;}i:250;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:7030;}i:251;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:91:"Le fléau de la dimension est un problème qui complexifie inutilement un jeu de données. ";}i:2;i:7033;}i:252;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:7124;}i:253;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:7124;}i:254;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:362:"L'ACP permet de réduire le nombre de dimensions d'un problème, en exprimant l'ensemble des données selon des axes, qui sont des combinaisons linéaires de toutes les autres variables. Ainsi chaque variable exprime un pourcentage de l'information totale ou variance totale (inertie), et l'objectif est de maximiser cette inertie pour gagner de l'information. 
";}i:2;i:7126;}i:255;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:7488;}i:256;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:7490;}i:257;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:7491;}i:258;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:7493;}i:259;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:17:":cpp:lamaface.jpg";i:1;s:0:"";i:2;N;i:3;s:3:"290";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:7494;}i:260;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:19:":cpp:lamaprofil.jpg";i:1;s:0:"";i:2;N;i:3;s:3:"400";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:7520;}i:261;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:7548;}i:262;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:7549;}i:263;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:7551;}i:264;a:3:{i:0;s:9:"linebreak";i:1;a:0:{}i:2;i:7552;}i:265;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:175:"
L'idée est de trouver le bon point de vue ou la variance du dataset sera maximisée. Ainsi, il y a un gain d'information et l'entrainement du modèle n'en sera que meilleur.";}i:2;i:7554;}i:266;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:7729;}i:267;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:7731;}i:268;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:37:"Construction du modèle de réduction";i:1;i:4;i:2;i:7731;}i:2;i:7731;}i:269;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:4;}i:2;i:7731;}i:270;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:7731;}i:271;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:178:"On commence tout d'abord par regarder quelle part de l'information est expliquée par chaque variable et combien de variables il nous faut pour expliquer 95 % de l'information.";}i:2;i:7776;}i:272;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:7954;}i:273;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:7954;}i:274;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:7956;}i:275;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:7958;}i:276;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:7969;}i:277;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:7971;}i:278;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:588:"
from sklearn.decomposition import PCA

model = PCA(n_components = 15)#Création du modèle de réduction sur toutes les variables du dataset
X_reduced = model.fit_transform(data)#Application de la réduction aux données d'entrainement
np.argmax(np.cumsum(model.explained_variance_ratio_)) > 0.95#Détermination des variables 

plt.figure(figsize=(12,6))#Définition de la taille du graphique
plt.plot(np.cumsum(model.explained_variance_ratio_))#Affichage des variances expliquées en fonction du nombre de variables

plt.xlabel("Nombres de variables")
plt.ylabel("Variance expliquée")
";i:1;s:6:"python";i:2;N;}i:2;i:7978;}i:279;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:7978;}i:280;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:8583;}i:281;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:8585;}i:282;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:8591;}i:283;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:8593;}i:284;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:192:"
library(explor)
library(factoMineR)

res.PCA = PCA(scale(data))#Création du modèle de réduction
explor(res.PCA)#Ouverture d'une fenêtre permettant de visualiser les variables importantes
";i:1;s:6:"python";i:2;N;}i:2;i:8600;}i:285;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:8809;}i:286;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:9:"Résultat";i:1;i:5;i:2;i:8809;}i:2;i:8809;}i:287;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:8809;}i:288;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:8809;}i:289;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:27:":cpp:variance_expliquee.png";i:1;s:73:"Variance expliquée en fonction du nombre de variables du jeu de données";i:2;s:6:"center";i:3;s:3:"600";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:8824;}i:290;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:8935;}i:291;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:1;i:2;i:8937;}i:2;i:1;i:3;s:3:";#;";}i:2;i:8937;}i:292;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:8940;}i:293;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Figure 5 :";}i:2;i:8942;}i:294;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:8952;}i:295;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:74:" Variance expliquée en fonction du nombre de variables du jeu de données";}i:2;i:8954;}i:296;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:3;i:2;i:8954;}i:2;i:3;i:3;s:74:" Variance expliquée en fonction du nombre de variables du jeu de données";}i:2;i:8954;}i:297;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:16:"divalign2_center";i:1;a:3:{i:0;s:6:"center";i:1;i:4;i:2;i:9028;}i:2;i:4;i:3;s:4:"
;#;";}i:2;i:9028;}i:298;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:9035;}i:299;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:6:"Source";i:1;i:5;i:2;i:9035;}i:2;i:9035;}i:300;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:9035;}i:301;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:9046;}i:302;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:9046;}i:303;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:9046;}i:304;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:9050;}i:305;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:92:"https://www.youtube.com/watch?v=FTtzd31IAOw&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=28";i:1;s:43:" Machine Learnia par Guillaume Saint-Cirgue";}i:2;i:9051;}i:306;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:9191;}i:307;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:9191;}i:308;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:9191;}i:309;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:9196;}i:310;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:26:"La sélection de variables";i:1;i:2;i:2;i:9196;}i:2;i:9196;}i:311;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:9196;}i:312;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:9196;}i:313;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:22:":cpp:selection_var.png";i:1;s:23:"Sélection de variables";i:2;s:6:"center";i:3;s:3:"700";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:9234;}i:314;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:9290;}i:315;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:9290;}i:316;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:164:"Il s'agit d'une étape importante dans le nettoyage des données. Après le travail d'encodage, un nombre trop élevé de variables peut entrainer deux problèmes :";}i:2;i:9292;}i:317;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:9457;}i:318;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:9457;}i:319;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:9457;}i:320;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:9457;}i:321;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:4:" Un ";}i:2;i:9461;}i:322;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:9465;}i:323;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:32:"sur-apprentissage (over-fitting)";}i:2;i:9467;}i:324;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:9499;}i:325;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:205:" de votre modèle de prédiction sur les données d'entrainement et de validation. Ce qui dégrade considérablement les performances de vote modèle lors de la généralisation sur de nouvelles données. ";}i:2;i:9501;}i:326;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:9706;}i:327;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:9706;}i:328;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:9706;}i:329;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:9707;}i:330;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:9707;}i:331;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:9707;}i:332;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:2:"  ";}i:2;i:9711;}i:333;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:9713;}i:334;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:79:"L'augmentation du temps d'apprentissage et de calibration des hyper-paramètres";}i:2;i:9715;}i:335;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:9794;}i:336;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:97:" de vote modèle. Ce qui peut être un problème lors du déploiement du modèle pour un client. ";}i:2;i:9796;}i:337;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:9893;}i:338;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:9893;}i:339;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:9893;}i:340;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:9893;}i:341;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:179:"Pour éviter ces problèmes il vous faudra donc utiliser des méthodes de sélection de variables, pour ne garder que celles qui apportent le plus d'informations à votre modèle.";}i:2;i:9895;}i:342;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10074;}i:343;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10074;}i:344;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:10076;}i:345;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:10088;}i:346;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:10089;}i:347;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:10091;}i:348;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:10101;}i:349;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:90:" On utilisera de nouveau, le dataset de mesures de gaz contenus dans l'alcool, disponible ";}i:2;i:3;i:3;s:90:" On utilisera de nouveau, le dataset de mesures de gaz contenus dans l'alcool, disponible ";}i:2;i:10103;}i:350;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:66:"https://archive.ics.uci.edu/ml/datasets/Alcohol+QCM+Sensor+Dataset";i:1;s:3:"ici";}i:2;i:10193;}i:351;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:37:". On considérera le premier relevé.";}i:2;i:3;i:3;s:37:". On considérera le premier relevé.";}i:2;i:10267;}i:352;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:10304;}i:353;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10312;}i:354;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:10315;}i:355;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:25:"Les tests de dépendances";i:1;i:3;i:2;i:10315;}i:2;i:10315;}i:356;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:10315;}i:357;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10315;}i:358;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:145:"Les tests statistiques sélectionnent les K variables explicatives, dont les scores de dépendance avec la variable cible sont les plus élevés.";}i:2;i:10350;}i:359;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10495;}i:360;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:10497;}i:361;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:16:"Le test de ANOVA";i:1;i:4;i:2;i:10497;}i:2;i:10497;}i:362;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:4;}i:2;i:10497;}i:363;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10497;}i:364;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:234:"Visualisation des scores de dépendances selon le test de ANOVA. Le résultat de ce test retourne deux tableaux : les scores de dépendances et la probabilité de rejeter l'hypothèse de dépendance alors qu'elle est vraie (p-valeur).";}i:2;i:10521;}i:365;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10755;}i:366;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10755;}i:367;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:10757;}i:368;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:10759;}i:369;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:10770;}i:370;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:10772;}i:371;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:239:"
from sklearn.feature_selection import SelectKBest, f_regression
 
X = data.drop(columns=['1-Octanol'])#Définition des variables explicatives
y = data['1-Octanol']#Définition de la variable cible
 
f_regression(X, y)#Test de dépendance
";i:1;s:6:"python";i:2;N;}i:2;i:10779;}i:372;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:10779;}i:373;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:11035;}i:374;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:11037;}i:375;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:11043;}i:376;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:11045;}i:377;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:66:"
y <- data$X1.Octanol
X <- data[, -c(11)]

aov(y ~., data = data)
";i:1;s:6:"python";i:2;N;}i:2;i:11052;}i:378;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:11135;}i:379;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:25:"Variables sélectionnées";i:1;i:5;i:2;i:11135;}i:2;i:11135;}i:380;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:11135;}i:381;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:11135;}i:382;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:202:"On sélectionne alors les k=5 variables ayant les scores les plus élevés selon le test de ANOVA, on applique la sélection sur l'ensemble des données puis on récupère les variables sélectionnées.";}i:2;i:11166;}i:383;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:11368;}i:384;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:11368;}i:385;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:11370;}i:386;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:11372;}i:387;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:11383;}i:388;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:11385;}i:389;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:214:"
selecteur = SelectKBest(f_regression, k=5)#Initialisation du selecteur
selecteur.fit_transform(X, y)#Application aux données 
np.array(X.columns)[selecteur.get_support()]#Récupération des variables conservées
";i:1;s:6:"python";i:2;N;}i:2;i:11392;}i:390;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:11392;}i:391;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:11623;}i:392;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:11635;}i:393;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:11636;}i:394;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:11638;}i:395;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:11648;}i:396;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:140:" Il existe une méthode selectKBest sous R, toutefois son implémentation avec la sélection par ANOVA impose des conditions particulières.";}i:2;i:3;i:3;s:140:" Il existe une méthode selectKBest sous R, toutefois son implémentation avec la sélection par ANOVA impose des conditions particulières.";}i:2;i:11650;}i:397;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:11790;}i:398;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:11798;}i:399;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:11798;}i:400;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:174:"Vous pouvez néanmoins sélectionner les variables les plus importantes en observant les F-valeur associées à chaque variable explicative, par rapport à la variable cible.";}i:2;i:11800;}i:401;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:11974;}i:402;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:37:"
summary.aov(aov(y ~., data = data))
";i:1;s:6:"python";i:2;N;}i:2;i:11981;}i:403;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:12035;}i:404;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:6:"Rappel";i:1;i:4;i:2;i:12035;}i:2;i:12035;}i:405;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:4;}i:2;i:12035;}i:406;a:3:{i:0;s:10:"table_open";i:1;a:3:{i:0;i:4;i:1;i:4;i:2;i:12049;}i:2;i:12048;}i:407;a:3:{i:0;s:15:"tablethead_open";i:1;a:0:{}i:2;i:12048;}i:408;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:12048;}i:409;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:12048;}i:410;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:12050;}i:411;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:12051;}i:412;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:12051;}i:413;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:9:" Chi2    ";}i:2;i:12052;}i:414;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:12061;}i:415;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:12061;}i:416;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:" ANOVA    ";}i:2;i:12062;}i:417;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:12072;}i:418;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:12072;}i:419;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:24:" Corrélation de Pearson";}i:2;i:12073;}i:420;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:12097;}i:421;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:12098;}i:422;a:3:{i:0;s:16:"tablethead_close";i:1;a:0:{}i:2;i:12098;}i:423;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:12098;}i:424;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:12098;}i:425;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:24:" Caractéristiques      ";}i:2;i:12100;}i:426;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:12124;}i:427;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:12124;}i:428;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:63:" Mesure la dépendance variable cible/explicatives et requiert ";}i:2;i:12125;}i:429;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:12188;}i:430;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:15:"obligatoirement";}i:2;i:12190;}i:431;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:12205;}i:432;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:36:" des données positives.            ";}i:2;i:12207;}i:433;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:12243;}i:434;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:12243;}i:435;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:172:" Vérifie s'il y a une différence significative entre plusieurs sous-échantillons (modalités) d'une même variable quantitative (le test compare des moyennes).          ";}i:2;i:12244;}i:436;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:12416;}i:437;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:12416;}i:438;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:63:"Reflète la relation linéaire entre deux variables continues. ";}i:2;i:12417;}i:439;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:12480;}i:440;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:12481;}i:441;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:12481;}i:442;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:12481;}i:443;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:19:" Evaluation ​    ";}i:2;i:12483;}i:444;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:12502;}i:445;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:12502;}i:446;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:91:"Plus le score de dépendance sera élevé, plus la variable dépendra de la variable cible.";}i:2;i:12503;}i:447;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:12594;}i:448;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:12594;}i:449;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:41:" Idem que pour le test du chi2.          ";}i:2;i:12595;}i:450;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:12636;}i:451;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:12636;}i:452;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:115:" Compris entre [-1;1] avec -1 corrélation négative (respectivement 1 pour positif) et 0 absence de corrélation.	";}i:2;i:12637;}i:453;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:12752;}i:454;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:12753;}i:455;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:12753;}i:456;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:12753;}i:457;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:20:" Utilisation ​    ";}i:2;i:12755;}i:458;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:12775;}i:459;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:12775;}i:460;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:"Classification";}i:2;i:12776;}i:461;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:12790;}i:462;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:12790;}i:463;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:40:" Classification et régression          ";}i:2;i:12791;}i:464;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:12831;}i:465;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;N;i:2;i:1;}i:2;i:12831;}i:466;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:12:"Régression ";}i:2;i:12832;}i:467;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:12844;}i:468;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:12845;}i:469;a:3:{i:0;s:11:"table_close";i:1;a:1:{i:0;i:12845;}i:2;i:12845;}i:470;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:12848;}i:471;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:7:"Sources";i:1;i:5;i:2;i:12848;}i:2;i:12848;}i:472;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:12848;}i:473;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:12861;}i:474;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:12861;}i:475;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:12861;}i:476;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:12865;}i:477;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:61:"http://​math.univ-lyon1.fr/​~duheille/​MASS42_anova.pdf";i:1;s:51:"Université de Lyon, département de mathématiques";}i:2;i:12866;}i:478;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:12983;}i:479;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:12983;}i:480;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:12983;}i:481;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:12983;}i:482;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:12987;}i:483;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:104:"http://​pagesped.cahuntsic.ca/​sc_sociales/​psy/​methosite/​consignes/​variance.htm#​quand";i:1;s:11:"​pagesped";}i:2;i:12988;}i:484;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:13108;}i:485;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:13109;}i:486;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:13109;}i:487;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:13109;}i:488;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:13109;}i:489;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:13113;}i:490;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:94:"https://​www.statisticshowto.com/​probability-and-statistics/​f-statistic-value-test/​";i:1;s:15:"statisticshowto";}i:2;i:13114;}i:491;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:13228;}i:492;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:13228;}i:493;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:13228;}i:494;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:13228;}i:495;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:13232;}i:496;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:92:"https://www.youtube.com/watch?v=T4nZDuakYlU&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=27";i:1;s:43:"Machine Learnia, par Guillaume Saint-Cirgue";}i:2;i:13233;}i:497;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:13373;}i:498;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:13373;}i:499;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:13373;}i:500;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:13375;}i:501;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:25:"Sélection via estimateur";i:1;i:3;i:2;i:13375;}i:2;i:13375;}i:502;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:13375;}i:503;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:13375;}i:504;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:244:"Cette méthode permet de sélectionner les variables de façon récursive, pour un estimateur particulier. Cela passe par plusieurs entrainements de l'estimateur, au cours desquels les variables les moins utiles sont à chaque fois éliminées.";}i:2;i:13410;}i:505;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:13654;}i:506;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:13654;}i:507;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:1;i:1;s:10:"alert-info";}i:2;i:1;i:3;s:12:"<alert info>";}i:2;i:13656;}i:508;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:1:" ";}i:2;i:3;i:3;s:1:" ";}i:2;i:13668;}i:509;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:13669;}i:510;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"Remarque";}i:2;i:13671;}i:511;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:13679;}i:512;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:98:" : Cette façon de faire nécessite la connaissance du principe de validation croisée, expliqué ";}i:2;i:3;i:3;s:98:" : Cette façon de faire nécessite la connaissance du principe de validation croisée, expliqué ";}i:2;i:13681;}i:513;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:51:"https://www.kaggle.com/alexisbcook/cross-validation";i:1;s:3:"ici";}i:2;i:13779;}i:514;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:3;i:1;s:66:". On considérera le même dataset que les parties précédentes. ";}i:2;i:3;i:3;s:66:". On considérera le même dataset que les parties précédentes. ";}i:2;i:13838;}i:515;a:3:{i:0;s:6:"plugin";i:1;a:4:{i:0;s:8:"alertbox";i:1;a:2:{i:0;i:4;i:1;s:0:"";}i:2;i:4;i:3;s:8:"</alert>";}i:2;i:13904;}i:516;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:13912;}i:517;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:13912;}i:518;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:13914;}i:519;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:13916;}i:520;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:13927;}i:521;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:13929;}i:522;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:457:"
from sklearn.feature_selection import RFECV
from sklearn.ensemble import RandomForestRegressor

selector = RFECV(RandomForestRegressor(random_state=0), step = 2, min_features_to_select = 5, cv=5)#On définit l'estimateur à tester, le nombre de variables à supprimer à chaque
#itérations (step), le nombre minimal de variables à garder à la fin et enfin le nombre de validation croisées à effectuer.
selector.fit(X, y)#Entraienement de l'estimateur
";i:1;s:6:"python";i:2;N;}i:2;i:13936;}i:523;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:13936;}i:524;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:14410;}i:525;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:14412;}i:526;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:14418;}i:527;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:14420;}i:528;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:475:"
library(caret)
library(randomForest)
library(e1071)


y <- data$X1.Octanol#Variable cible
y <- as.factor(y)#Transformation de la variable cible en facteur
X <- data[, -c(11)]#Variables explicatives

set.seed(0)#On initialise l'aléatoire pour la répétabilité 
selector <- rfeControl(functions = rfFuncs, method = 'cv', repeats = 5)#Définition des paramètres de controle de la sélection
select <- rfe(X, y, sizes = c(5), rfeControl = selector)#Sélection des variables
";i:1;s:6:"python";i:2;N;}i:2;i:14427;}i:529;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:14919;}i:530;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:25:"Variables sélectionnées";i:1;i:5;i:2;i:14919;}i:2;i:14919;}i:531;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:14919;}i:532;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:14919;}i:533;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:14950;}i:534;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Code Python";}i:2;i:14952;}i:535;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:14963;}i:536;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:14965;}i:537;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:234:"
selector.ranking_#Classement final des variables par ordre d'importance
selector.grid_scores_#Score de l'estimateur à chaque itération de l'algorithme
np.array(X.columns)[selector.get_support()]#Affichage des variables conservées
";i:1;s:6:"python";i:2;N;}i:2;i:14972;}i:538;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:14972;}i:539;a:3:{i:0;s:14:"underline_open";i:1;a:0:{}i:2;i:15223;}i:540;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:6:"Code R";}i:2;i:15225;}i:541;a:3:{i:0;s:15:"underline_close";i:1;a:0:{}i:2;i:15231;}i:542;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:15233;}i:543;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:43:"
select$optVariables#Variables conservées
";i:1;s:6:"python";i:2;N;}i:2;i:15240;}i:544;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:15300;}i:545;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:9:"Résultat";i:1;i:5;i:2;i:15300;}i:2;i:15300;}i:546;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:15300;}i:547;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:15300;}i:548;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:14:":cpp:rfecv.png";i:1;s:23:"Résultat de sélection";i:2;s:6:"center";i:3;s:3:"600";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:15315;}i:549;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:15363;}i:550;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:15363;}i:551;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:187:"Pour cet estimateur, la première variable est 3e dans le classement d'importance, tandis que les deuxième, troisième et quatrième variables sont parmi les premières plus importantes.";}i:2;i:15365;}i:552;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:15552;}i:553;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:15552;}i:554;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:365:"On voit aussi qu'à la première itération, le score de validation de 60 %, puis est passé à 80 % lorsqu'on supprime les deux variables les plus inutiles. Le score ne change pas 
après suppression de quatre autres variables, mais on voit une perte de qualité lorsqu'on veut en supprimer huit. C'est pourquoi finalement on a six variables en première position.";}i:2;i:15554;}i:555;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:15919;}i:556;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:15921;}i:557;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:7:"Sources";i:1;i:5;i:2;i:15921;}i:2;i:15921;}i:558;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:5;}i:2;i:15921;}i:559;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:15933;}i:560;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:15933;}i:561;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:15933;}i:562;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:15937;}i:563;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:51:"https://www.kaggle.com/alexisbcook/cross-validation";i:1;s:6:"Kaggle";}i:2;i:15938;}i:564;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:16000;}i:565;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:16000;}i:566;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:16000;}i:567;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:16000;}i:568;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:16004;}i:569;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:92:"https://www.youtube.com/watch?v=T4nZDuakYlU&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=27";i:1;s:43:"Machine Learnia, par Guillaume Saint-Cirgue";}i:2;i:16005;}i:570;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:16145;}i:571;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:16145;}i:572;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:16145;}i:573;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:16147;}i:574;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:25:"Les règles d'association";i:1;i:2;i:2;i:16147;}i:2;i:16147;}i:575;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:16147;}i:576;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:16147;}i:577;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:237:"Nous allons travailler avec un DataSet qui recense plusieurs tickets de caisse par pays. Nous avons un peu pré-traité le dataset pour qu'il soit directement utilisable pour la construction de règle. Vous trouverez le dataset utilisé ";}i:2;i:16184;}i:578;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:121:"https://github.com/LlamasPartan/Machine_Learning_Ressource/tree/master/Exploration%20des%20donnees/Data%20ticket%20caisse";i:1;s:4:" ici";}i:2;i:16421;}i:579;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:50:" ainsi que le code qui a permi de le pré-traité.";}i:2;i:16551;}i:580;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:16601;}i:581;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:16601;}i:582;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:26:":cpp:association_rules.png";i:1;s:21:"Règles d'association";i:2;s:6:"center";i:3;s:3:"800";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:16603;}i:583;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:16661;}i:584;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:16661;}i:585;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:222:"On va voir comment faire des règles d'association entre les éléments de tickets de caisse. Le but étant de faire ressortir les combinaisons de produits les plus courants, afin de proposer de possibles achats associés.";}i:2;i:16663;}i:586;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:16885;}i:587;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:16887;}i:588;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:42:"Forme des données et spécificité Python";i:1;i:3;i:2;i:16887;}i:2;i:16887;}i:589;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:16887;}i:590;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:16887;}i:591;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:160:"Il faut avant tout que les données aient une forme particulière, par exemple que chacun des produits et des tickets de caisse soient reliés par un même ID. ";}i:2;i:16939;}i:592;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:17100;}i:593;a:3:{i:0;s:10:"table_open";i:1;a:3:{i:0;i:3;i:1;i:4;i:2;i:17101;}i:2;i:17100;}i:594;a:3:{i:0;s:15:"tablethead_open";i:1;a:0:{}i:2;i:17100;}i:595;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:17100;}i:596;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17100;}i:597;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:"              ";}i:2;i:17102;}i:598;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:17116;}i:599;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17116;}i:600;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:" ID Ticket    ";}i:2;i:17117;}i:601;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:17131;}i:602;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17131;}i:603;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:12:" Produit    ";}i:2;i:17132;}i:604;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:17144;}i:605;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:17145;}i:606;a:3:{i:0;s:16:"tablethead_close";i:1;a:0:{}i:2;i:17145;}i:607;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:17145;}i:608;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17145;}i:609;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:" 1      ";}i:2;i:17147;}i:610;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:17155;}i:611;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17155;}i:612;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:" 1            ";}i:2;i:17156;}i:613;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:17170;}i:614;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17170;}i:615;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:19:" Pêche            ";}i:2;i:17171;}i:616;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:17190;}i:617;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:17191;}i:618;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:17191;}i:619;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17191;}i:620;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:" 2      ";}i:2;i:17193;}i:621;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:17201;}i:622;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17201;}i:623;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:" 1            ";}i:2;i:17202;}i:624;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:17216;}i:625;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17216;}i:626;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:18:" Prune            ";}i:2;i:17217;}i:627;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:17235;}i:628;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:17236;}i:629;a:3:{i:0;s:13:"tablerow_open";i:1;a:0:{}i:2;i:17236;}i:630;a:3:{i:0;s:16:"tableheader_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17236;}i:631;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:" 3      ";}i:2;i:17238;}i:632;a:3:{i:0;s:17:"tableheader_close";i:1;a:0:{}i:2;i:17246;}i:633;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17246;}i:634;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:14:" 2            ";}i:2;i:17247;}i:635;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:17261;}i:636;a:3:{i:0;s:14:"tablecell_open";i:1;a:3:{i:0;i:1;i:1;s:4:"left";i:2;i:1;}i:2;i:17261;}i:637;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:18:" Abricot          ";}i:2;i:17262;}i:638;a:3:{i:0;s:15:"tablecell_close";i:1;a:0:{}i:2;i:17280;}i:639;a:3:{i:0;s:14:"tablerow_close";i:1;a:0:{}i:2;i:17281;}i:640;a:3:{i:0;s:11:"table_close";i:1;a:1:{i:0;i:17281;}i:2;i:17281;}i:641;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:17281;}i:642;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:149:"Ensuite il est nécessaire de rendre le dataset binaire. Il y aura des 0 quand le produit n'est pas présent dans le ticket et 1 s’il est présent.";}i:2;i:17283;}i:643;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:17432;}i:644;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:178:"
import pandas as pd
dodo = pd.read_csv(("produit.csv"))
#tableau croisé 0/1
DataFrame = pd.crosstab(dodo.NumTicket,dodo.Produit)
DataFrame.columns #Permet d'avoir les colonnes
";i:1;s:6:"python";i:2;N;}i:2;i:17439;}i:645;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:17439;}i:646;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:127:"On peut appliquer l'algorithme qui calcule à chaque fois le support et élimine les combinaisons dont le support est trop bas.";}i:2;i:17634;}i:647;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:17761;}i:648;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:216:"
#importation de la fonction apriori
from mlxtend.frequent_patterns import apriori
 
#itemsets frequents
freq_itemsets = apriori(DataFrame,min_support=0.025,max_len=3, use_colnames=True)
freq_itemsets.iloc[60:80, :]
";i:1;s:6:"python";i:2;N;}i:2;i:17768;}i:649;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:17768;}i:650;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:18001;}i:651;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Résultat :";}i:2;i:18003;}i:652;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:18014;}i:653;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18016;}i:654;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18016;}i:655;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:28:":cpp:resultatassociation.png";i:1;s:0:"";i:2;s:6:"center";i:3;s:3:"250";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:18018;}i:656;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18057;}i:657;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18057;}i:658;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:18059;}i:659;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:10:"Remarque :";}i:2;i:18061;}i:660;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:18071;}i:661;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18073;}i:662;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18073;}i:663;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:220:"Parfois, on obtient trop de règles et elles ne sont pas exploitables c'est pourquoi il peut être intéressant de faire ressortir certaines règles. Pour cela, il existe une librairie particulière utilisée en Python :";}i:2;i:18076;}i:664;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18296;}i:665;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:231:"
#fonction de calcul des règles
from mlxtend.frequent_patterns import association_rules

#génération des règles à partir des itemsets fréquents
regles = association_rules(freq_itemsets,metric="confidence",min_threshold=0.75)
";i:1;s:6:"python";i:2;N;}i:2;i:18303;}i:666;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:18550;}i:667;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:40:"Forme des données et spécificité en R";i:1;i:3;i:2;i:18550;}i:2;i:18550;}i:668;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:3;}i:2;i:18550;}i:669;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18550;}i:670;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:158:"En R, il est possible de trouver les règles sur le dataFrame brut. Commençon par charger les données et les librairies qui vont nous permettre de le faire.";}i:2;i:18600;}i:671;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18758;}i:672;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:89:"
library(arules)
library(arulesViz)
data = read.csv("produitPourR.csv")
data = data[,-1]
";i:1;s:6:"python";i:2;N;}i:2;i:18765;}i:673;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18765;}i:674;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:98:"On peut ensuite lancer l'algorithme et contrairement à Python visualiser les règles construites.";}i:2;i:18871;}i:675;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:18969;}i:676;a:3:{i:0;s:4:"code";i:1;a:3:{i:0;s:173:"
rules = apriori(data, parameter = list(support=0.01, confidence=0.5, minlen=2))
inspect(sort(rules,by="confidence"))
plot (rules[1:19],method="graph",shading="confidence")
";i:1;s:6:"python";i:2;N;}i:2;i:18976;}i:677;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:18976;}i:678;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:19166;}i:679;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:11:"Résultat :";}i:2;i:19168;}i:680;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:19179;}i:681;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:19181;}i:682;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:19181;}i:683;a:3:{i:0;s:13:"internalmedia";i:1;a:7:{i:0;s:24:":cpp:association.png.png";i:1;s:0:"";i:2;s:6:"center";i:3;s:3:"700";i:4;N;i:5;s:5:"cache";i:6;s:7:"details";}i:2;i:19183;}i:684;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:19218;}i:685;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:19218;}i:686;a:3:{i:0;s:11:"strong_open";i:1;a:0:{}i:2;i:19221;}i:687;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:8:"Source :";}i:2;i:19223;}i:688;a:3:{i:0;s:12:"strong_close";i:1;a:0:{}i:2;i:19231;}i:689;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:0:"";}i:2;i:19233;}i:690;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:19234;}i:691;a:3:{i:0;s:10:"listu_open";i:1;a:0:{}i:2;i:19234;}i:692;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:19234;}i:693;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:19234;}i:694;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:19238;}i:695;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:89:"http://eric.univ-lyon2.fr/~ricco/tanagra/fichiers/fr_Tanagra_Python_Association_Rules.pdf";i:1;N;}i:2;i:19239;}i:696;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:19332;}i:697;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:19332;}i:698;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:19332;}i:699;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:19332;}i:700;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:19336;}i:701;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:68:"http://eric.univ-lyon2.fr/~ricco/cours/slides/regles_association.pdf";i:1;N;}i:2;i:19337;}i:702;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:19409;}i:703;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:19409;}i:704;a:3:{i:0;s:13:"listitem_open";i:1;a:1:{i:0;i:1;}i:2;i:19409;}i:705;a:3:{i:0;s:16:"listcontent_open";i:1;a:0:{}i:2;i:19409;}i:706;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:" ";}i:2;i:19413;}i:707;a:3:{i:0;s:12:"externallink";i:1;a:2:{i:0;s:96:"https://freres.peyronnet.eu/trouver-la-bonne-association-a-laide-du-data-miningmachine-learning/";i:1;N;}i:2;i:19414;}i:708;a:3:{i:0;s:17:"listcontent_close";i:1;a:0:{}i:2;i:19514;}i:709;a:3:{i:0;s:14:"listitem_close";i:1;a:0:{}i:2;i:19514;}i:710;a:3:{i:0;s:11:"listu_close";i:1;a:0:{}i:2;i:19514;}i:711;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:19516;}i:712;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:10:"Conclusion";i:1;i:2;i:2;i:19516;}i:2;i:19516;}i:713;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:19516;}i:714;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:19516;}i:715;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:359:"Avoir une idée claire de ses données, c'est savoir quelles sont les variables à  ajuster ou à créer pour améliorer son apprentissage. Il s'agit d'une étape primordiale pour mieux appréhender le problème donné et y apporter une réponse plus adapter. Ainsi, avec toutes les techniques présentés vous avez les outils pour mieux aborder le problème.";}i:2;i:19538;}i:716;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:19897;}i:717;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:19897;}i:718;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:19897;}}