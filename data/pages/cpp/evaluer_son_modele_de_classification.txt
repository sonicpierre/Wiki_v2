[[cpp:IA| Machine Learning]]

{{ :cpp:evaluation.jpg?500 |}}

On peut ensuite évaluer le modèle pour avoir une meilleur idée de sa précision. L'évaluation est une étape indispensable pour tout travail de Data Science autant pour les travaux de régression que les modèles de classification. Il est bon de pouvoir tester plusieurs methodes pour évaluer son modèle car <color #ed1c24>**toutes les précisions ne se valent pas**</color>.

<alert info>**DataSet :** Nous allons 2 datasets, un premier pour entraîner des algorithmes de régression concernant le diabète et un second concernant la reconnaissance de lettres manuscrites. </alert> 

=====Les différentes erreurs=====

Il est important de séparer son dataset correctement pour éviter la fuite de donnée (data leak) qui consisterait à entraîner des modèles sur un ensemble qui servirait ensuite à le tester.

====L'erreur d'ajustement/prédiction====

On peut distinguer deux sortes d'erreur :

  * **Erreur d'ajustement** : Il s'agit de l'erreur commise sur les données qui ont servi à l'entrainement du modèle. 
  * **Erreur de prédiction** : Il s'agit de l'erreur commise sur des données qui n'ont jamais été vu par le modèle. 

Il est intéressant de les calculer et de les comparer pour voir si notre modèle est en overfitting. Prenons par exemple un modèle qui a tendance à over-fiter facilement, l'arbre de régression.

On commence par couper en 2 le dataset pour séparer ainsi le partie pour l'entrainement et le test :

__En Python :__

<code python>
from sklearn.model_selection import train_test_split
data_train, data_test = train_test_split(data, test_size = 0.2, random_state=0)

data_train_train = data_train.drop('target', axis=1)
data_train_test = data_train.target
data_test_train = data_test.drop('target', axis=1)
data_test_test = data_test.target
</code>

__En R :__

<code python>

</code>

On peut ensuite entraîner un premier modèle d'arbre sans mettre d'hyper paramètre et calculer l'erreur d'ajustement. Nous utiliserons ici la RMSE (Root Mean Squared Error) pour approximer l'erreur et la comparer :

__En Python :__

<code python>
tree_mod = DecisionTreeRegressor()
tree_mod.fit(data_train_train, data_train_test)

from sklearn.metrics import mean_squared_error
print(mean_squared_error(tree_mod.predict(data_train_train), data_train_test))
print(mean_squared_error(tree_mod.predict(data_test_train), data_test_test))
</code> 

__En R :__

<code python>

</code>

Vous remarquerez que l'erreur d'ajustement est bien plus faible que l'erreur de précision. On peut donc dire que le modèle est en over fitting et se généralise mal. On peut bien entendu sur la profondeur de l'arbre pour palier à ce problème.

On peut visualiser l'évolution de ces 2 courbes en fonction du nombre de données leur sont fournies :

__En Python :__

<code python>
from sklearn.model_selection import learning_curve

N , train_score, val_score = learning_curve(DecisionTreeRegressor(),data.drop('target', axis = 1), data['target'], cv = 5, scoring='neg_mean_squared_error')
plt.plot(N, val_score.mean(axis=1), label='Erreur précision' )
plt.plot(N, train_score.mean(axis=1), label='Erreur entrainement')
plt.title("Arbre de Régression")
plt.legend()
</code>

__En R :__

<code python>

</code>

**Résultat :**

On a fait varier les différents modèles pour voir comment évolue les erreurs d'ajustement et de précision.

{{ :cpp:erreur_prediction_ajustement.png?800 |}}

**Source :**
  * [[http://www.statsoft.fr/concepts-statistiques/qualite-d-ajustement/qualite-d-ajustement.htm]]
  * [[https://openclassrooms.com/fr/courses/4297211-evaluez-les-performances-dun-modele-de-machine-learning/4308241-mettez-en-place-un-cadre-de-validation-croisee]]

====La validation croisée====

Couper son dataset c'est toujours se priver d'une part des données sur lesquelles on ne peut pas tester ni ajuster son modèle. Lors du réglage des hyperparamètres, il faut faire attention à ne pas rendre performant le modèle que sur un jeux d'entrainement précis le rendant ainsi peu généralisable (overfitting).

{{ :cpp:validation_croise.png?500 |}}

On fait donc varier la <color #7092be>**partie du jeux de donnée permettant l'entraînement**</color> et le <color #ed1c24>**test du modèle**</color>. On respecte bien entendu le fait de ne jamais entraîner un modèle sur l'ensemble où on le teste et on a ainsi une idée plus globale de l'erreur.

La découpe "classique" ne prend pas forcément en compte le fait que les classes soient déséquilibrées.

{{ :cpp:stratifiation.png?500 |}}

La validation croisée lors de la découpe en K sous ensembles doit respecter les proportions de chacune des classes on appelle ce principe la stratification. Heureusement cette problématique est traité automatiquement dans les librairies permettant la mise en place de la validation croisée.

Illustrons cette technique en entraînant un régression linéaire et l'évaluant grâce à une validation croisée :

__En Python :__

<code python>
from sklearn.model_selection import cross_val_score

lin_mod = LinearRegression()
scores = cross_val_score(lin_mod, data_train, data_etiquette, scoring="neg_mean_squared_error", cv = 5)
lin_rmse = np.sqrt(-scores)
</code>

__En R :__

<code python>

</code>

**Résultat :**

array([54.44845301, 51.5418087 , 55.78449685, 51.32305279, 55.17741631])

Il est intéressant de calculer la moyenne avec **.mean()** et l'écart type avec **.std()** pour avoir une idée des variations de cette erreur en fonction des données. Ici on obtient **moyenne = 53.65** et **écart type = 1.86**.

<alert info>**Info :** Il existe une méthode appelée **leave-one out** qui conciste à couper le jeux de données en n fois. On teste donc sur une seule observation les modèles entraînés sur l'ensemble du dataset moins cette observation. On peut appliquer ce principe si il n'y a pas très peu d'observation et si le modèle est peu couteux à entraîner.</alert>
=====Les erreurs de classification=====

La construction de ce score peut se faire en une ligne de code, cependant il est important de comprendre ce qu'il se passe derrière. La construction d'une matrice de confusion et les notions de rappel et précision sont essantielles pour bien comprendre tous les aspects de score F1 c'est pourquoi nous allons les aborder.

====Les matrices de confusion====

{{ :cpp:matricedeconfusion.png?600 |}}

Plus la matrice est proche d'une matrice diagonale et plus le modèle est bon.

__En Python :__

<code python>
from sklearn.metrics import confusion_matrix
data_Test = pd.read_csv("test_Titanic_py.csv")

data_train= data_Test.drop("Survived", axis = 1)
data_test = data_Test.Survived

prediction = tree_clf.predict(data_train)
confusion_matrix(prediction, data_test)
</code>

__En R :__

<code python>
library(caret)
data_test <- read.csv("test_Titanic_R.csv", row.names = 1)
#On fait les prédictions
class_prediction <- predict(object = model,
                            newdata = data_test,
                            type = "class")
#On construit la matrice
matrice_de_Confusion <- table(data_test$Survived, class_prediction)
</code>

**Résultat :**
On obtient ainsi la matrice de confusion suivante :

$$
\begin{pmatrix}
274 & 4\\
12 & 114
\end{pmatrix}
$$

====La précision et le rappel====

Évaluer un modèle de classification n'est pas forcément une chose aisée tout dépend du résultat voulu :

{{ :cpp:precisionrappel.png?800 |}}

Il y a un compromis à avoir entre **Rappel** et **Précision**. Une forte précision implique un rappel plus faible et vice versa. Tout dépend des désirs du clients et du problème traité.
\\
Dans le cas du Titanic, vous voulez peut-être monter à bord que si vous êtes sûr de survivre donc il faut un modèle avec une forte précision.

**Théorie :**

Les scores de rappel et de précision sont calculés grace à la matrice de confusion vu précédemment. 

  * Précision : $$\frac{Vrai Positif}{Vrai Positif + Faux Positif}$$
\\
\\
  * Rappel : $$\frac{Vrai Positif}{Vrai Positif + Faux Négatif}$$

__En Python :__

<code python>
from sklearn.metrics import precision_score, recall_score
precision_score(prediction, data_test)
recall_score(prediction, data_test)
</code>

__En R :__

<code python>
precision <- matrice_Confusion[4]/(matrice_Confusion[4] + matrice_Confusion[3])
rappel <- matrice_Confusion[4]/(matrice_Confusion[4] + matrice_Confusion[2])
</code>

**Source :**
  * [[https://apiacoa.org/blog/2014/02/initiation-a-rpart.fr.html]]
====Score F1====

Ce score est coramment utilisé en classification pour comparer rapidement 2 modèles de classification. Il favorise un rappel et une précision élevée, si l'un des deux est plus faible alors le score s'en verra amoindri.

$$F1 = 2 \times \frac{précision \times rappel}{précision + rappel}$$

Appliquons cette mesure à notre modèle du Titanic :

__Code Python__

<code python>
from sklearn.metrics import f1_score
f1_score(prediction, data_test)
</code>

__Code R__

<code python>
F1 <- (2 * precision * rappel) / (precision + rappel)
</code>

====Le Caveats====

**Source :**
  * [[https://www.it-swarm.dev/fr/r/un-moyen-facile-de-compter-la-precision-le-rappel-et-le-score-f1-en-r/941171789/]]

====La courbe ROC====

La courbe ROC (Receiver-Operator Characteristic) 

**Source :**
  * [[https://openclassrooms.com/fr/courses/4297211-evaluez-les-performances-dun-modele-de-machine-learning/4308261-evaluez-un-algorithme-de-classification-qui-retourne-des-scores]]
  * [[https://www.kdnuggets.com/2019/10/5-classification-evaluation-metrics-every-data-scientist-must-know.html]]

====La courbe PR====