[[cpp:Natural Langage Processing | Natural Langage Processing]]

{{:cpp:preprocessing.jpg?500|}}

Nous allons traiter ici du preprocessing pour les données textuelles dans le but de faire une analyse de fréquence des mots et visualiser notre corpus.
\\
\\
===Librairies nécessaires===

^ Librairie Python ^ Package R    ^ Utilité ^
| nltk             | tm, NLP       | Texte mining    |
| nltk             | tidytext     | ngrams          |
| nltk             | bind_tf_idf  | tf-idf          |
| spacy ou nltk    | SnowballC    | Stemming        |
| wordcloud        | wordcloud    |Nuage de mots    |
| seasborn         | RColorBrewer | Affichage de graphiques |

=====Nettoyage des données=====

Dans un texte, tout n'est pas important c'est pourquoi il est nécessaire de nettoyer les données, afin de garder, en priorité, les mots qui apportent le plus d'informations. On va voir ici cinq méthodes parmi les plus courantes :

  * Tokénisation
  * Normalisation du texte
  * Suppression des stopwords
  * Lemmatization (ou Stemming)
  * N_grams

Pour les exemples, nous utiliserons la phrase :

<code python>
phrase = "JE suiS Un ABruti Et, je vAis t'écrire quelque CHosE !!"
</code>

====Tokénisation====

Il s'agit de couper une phrase dans le but de construire une liste avec, à chaque indice (place dans la liste), un mot ce qui facilite grandement l'apprentissage de la machine. Il est nécessaire de prendre le tokéniseur adapté à la langue (ici ce sera le français).

__Code Python :__

<code python>
tokenizer = nltk.RegexpTokenizer(r'\w+')
tokenizer.tokenize(phrase)
</code>

__Équivalent R__

Il n'y a pas de construction directe de listes comme en Python. Il s'agit plutôt de corriger et de filtrer les caractères non voulus. 

<code python>
#Remplace tous les caractères spéciaux
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

# Supprimer les ponctuations
docs <- tm_map(docs, removePunctuation)
# Supprimer les espaces vides supplémentaires
docs <- tm_map(docs, stripWhitespace)
# Supprimer les nombres
docs <- tm_map(docs, removeNumbers)
</code>

**Résultat :**

<nowiki>
["JE", "suiS", "Un", "ABruti", "Et", "je", "vAis", "t", "écrire", "quelque", "CHosE"]
</nowiki>

Ici, il y a même un premier traitement qui enlève certaines ponctuations.

====Normalisation du texte====

La normalisation du texte revient à mettre tous les caractères en minuscules afin de limiter la diversité du texte et gagner en simplicité.

__Code python__

<code python>
# Convertir le texte en minuscule
phrase.lower()
</code>

__Équivalent R__

<code python>
docs <- tm_map(docs, content_transformer(tolower))
</code>

**Résultat :**

<nowiki>
"je suis un abruti et, je vais t'écrire quelque chose !!"
</nowiki>

====Suppression des stopwords====

Les stopwords sont des mots qui n'apportent rien à l'apprentissage du modèle. Il en existe déjà par défaut dans la librairie **nltk** mais, bien entendu, on peut  en rajouter pour être plus en accord avec son étude.

__Code Python :__

<code python>
#Ici dataFrame['Commentaires'] est une phrase sous forme de liste après tokénisation.
def suppressionDesStopWords(dataFrame):
    stopW = stopwords.words('french')
    for i in range(dataFrame.shape[0]):
        dataFrame['Commentaires'][i] = [word for word in dataFrame['Commentaires'][i] if word not in stopW]
</code>

__Équivalent R :__

<code python>
# Supprimer les mots vides anglais
docs <- tm_map(docs, removeWords, stopwords("english"))
</code>

**Résultat :**

<nowiki>
['abruti', 'vais', 'écrire', 'quelque', 'chose']
</nowiki>

Pour rajouter des mots aux stopwords on utilise :

__Code Python :__

<code python>
stopsW.extend(["ma", "poule"])
</code>

__Équivalent R :__

<code python>
# Supprimer votre propre liste de mots non désirés
docs <- tm_map(docs, removeWords, c("ma", "poule")) 
</code>

Il est donc nécessaire de prendre le temps de modifier son panel de stopwords en fonction du problème donné.

====Stemming====

Il s'agit ici de //raciner// les mots pour ne garder que leur radical. C'est un processus assez rapide qui enlève les suffixes et les préfixes et apparaît actuellement comme une solution viable pour la langue française.

__Code Python :__

<code python>
from nltk.stem.snowball import FrenchStemmer
stemmer = FrenchStemmer()
phrase = stemmer.stem(phrase)
</code>

__Equivalent R :__

<code python>
# Text stemming
docs <- tm_map(docs, stemDocument)
</code>

**Résultat :**

<nowiki>
"je suis un abruti et je vais t'écrire quelque chos"
</nowiki>

====Lemmatization====

Ce processus consiste à transformer tous les noms en leur sigulier et tous les verbes en leur infinitif. Cependant ce procédé **n'existe pas en français**.

<code python>
from nltk import word_tokenize
from nltk.stem.wordnet import WordNetLemmatizer 
lemma = WordNetLemmatizer()
text = word_tokenize('The girls want to play with their parents')
[lemma.lemmatize(word) for word in text]
</code>

**Résultat :**

<nowiki>
["The", "girl", "want", "to", "play", "with", "their", "parent"]
</nowiki>

====N_grams====

Cette méthode consiste à garder les suites de mots à la place du mot seul. On peut ainsi obtenir, pour chaque mot, celui qui suit juste derrière de façon simple. Cette approche permet de saisir de façon minimale le contexte dans lequel le mot est employé.

__Code Python :__

<code python>
from nltk import ngrams
from nltk import word_tokenize
tokens = word_tokenise(phrase)
bigrams = ngrams(tokens, 2)
for words in bigrams:
    print(words)
</code>

__En R :__

<code python>
library(tidytext)
#Bigram
book_bigram <- data_frame(line = 1:nrow(book), text = book$text)  %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)
</code>

**Résultat :**

<nowiki>
('Je', 'suis')
('suis', 'un')
('un', 'abruti')
('abruti', 'et')
('et', 'je')
('je', 'vais')
('vais', "t'écrire")
("t'écrire", 'quelque')
('quelque', 'chose')
</nowiki>

**Sources**

[[https://www.actuia.com/contribution/victorbigand/tutoriel-tal-pour-les-debutants-classification-de-texte/]]

[[https://openclassrooms.com/fr/courses/4470541-analysez-vos-donnees-textuelles/4855006-effectuez-des-plongements-de-mots-word-embeddings]]

[[http://www.sthda.com/french/wiki/text-mining-et-nuage-de-mots-avec-le-logiciel-r-5-etapes-simples-a-savoir]]

[[https://thinkr.fr/text-mining-n-gramme-avec-r/#bigramme,_trigramme_et_quadrigramme]]
=====Visualiser les données=====

Ici on va aborder les différentes méthodes pour visualiser le texte nettoyé. Pour cela on utilisera les librairies wordcloud et seaborn.

On part du principe que le texte a été nettoyé avec les méthodes ci-dessus et qu'il ne reste plus qu'une seule chaîne de caractères que l'on nomme corpus.

Pour visualiser le texte de façon graphique, il y a plusieurs méthodes :

====Les wordCloud====

__Code Python :__

<code python>
wordcloud = WordCloud(width=800, height=500,
                      random_state=21, max_font_size=110).generate(phrase)
plt.figure(figsize=(15, 12))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off');
</code>

__Équivalent R__

<code python>
#On commence par calculer la matrice des mots

dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)

</code>

^ word          ^ freq      ^
| will          | 17        |
| freedom       | 13        |
| ring          | 12        |
| day           | 11        |
| dream         | 11        |

<code python>
#On construit le bon graphique

set.seed(42)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
</code>

{{ :cpp:nuagedemots.png?400 |}}

Ainsi, on voit clairement les mots qui ressortent du texte et on a une meilleure idée de la communauté à laquelle on a affaire.

====Visualiser les fréquences dans un histogramme====

Il est parfois nécessaire de mieux comprendre la fréquence d'apparition des mots et pour cela un histogramme peut être une bonne solution :

__Code Python :__

<code python>
from collections import Counter
import seaborn as sns

eap_list = recompositionDuText(data).split()
eap_counts = Counter(eap_list)
eap_common_words = [word[0] for word in eap_counts.most_common(25)]
eap_common_counts = [word[1] for word in eap_counts.most_common(25)]

plt.style.use('dark_background')
plt.figure(figsize=(15, 12))

sns.barplot(x=eap_common_words, y=eap_common_counts)
plt.title('Fréquence des mots');
</code>

__Équivalent R__

<code python>

barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")
</code>

{{ :cpp:histogrammedesmots.png?400 |}}

====Visualiser la richesse lexicale====

Il est parfois intéressant de faire ressortir les mots les plus utilisés, les moins utilisés ou encore ceux d'une certaine longueur. Nous allons ici essayer de faire ressortir ces différents mots pour mieux comprendre le texte qui les contient. Pour illustrer nos propos, nous utiliserons un texte déjà présent dans les librairies.

__Python :__

<code python>
from nltk.book import *
#Pour Python, il y a différents textes avec différents type de langages, nous continuerons avec ces textes
</code>

__R :__

<code python>
library("proustr")
books <- proust_books()
</code>

On va maintenant comparer 2 types de diversités lexicales :

**Diversité texte**

__Python :__

<code python>
def diversiteLexicale(text):
    return len(text) / len(set(text))
diversiteLexicale(text3)
</code>

**Diversité d'un mot**

__Python :__

<code python>
def pourcentage(compte, total):
    return 100 * compte / total
pourcentage(text3.count('a'), len(text3))
</code>

Maintenant, faisons ressortir les mots avec certaines caractéristiques : 

**Les mots d'une certaine longueur**

__Python :__

<code python>
V = set(text1)
mots_longs = [w for w in V if len(w) > 15]
sorted(mots_longs)
</code>

**Résultat :**

['CIRCUMNAVIGATION',
 'Physiognomically',
 'apprehensiveness',
 'cannibalistically',....

**Les mots n'apparaissant qu'une seule fois (hapax)**

<code python>
fdist = FreqDist(text3)
fdist.hapaxes()
</code>

**Résultat :**

['form',
 'void',
 'Day',
 'Night',
 'firmame'...
=====Corrélation entre mots=====

On peut regarder de façon simple quel mot est couramment associé à un autre mot. Bien évidemment, avant de faire cette étude, il faut avoir récupéré la matrice de corrélation des mots. On va chercher les corrélations entre le mot //freedom// et les autres.

__Code R__

<code python>
findAssocs(dtm, terms = "freedom", corlimit = 0.3)
</code>

__Équivalent en Python__

Il n'existe pas d'équivalent direct en Python mais il est possible de proposer de passer par les règles d'association. Voir cours Machine Learning

**Résultat :**

^ word          ^ Corrélation   ^
| will          | 0.5            |
| freedom       | 0.3            |
| ring          | 0.05           |
| day           | 0.1            |
| dream         | 0.1            |

=====Tf-IDF=====

Le calcul Tf-IDF (term frequency-inverse document frequency) permet de calculer un score de proximité entre un terme recherché et un document. 

**Théorie :**

La partie Tf calcule une fonction croissante de la fréquence du terme recherché dans le document d'étude.
 
$$TF(i) = \frac{\log_{2}(Freq(i,j) + 1)}{\log_{2}(L_{j})}$$

^ Termes         ^ Signification                         ^
| $Freq(i,j)$    | Fréquence du mt i dans le document j  |         
| $L_{j}$        | Nombre de mots dans le document j     |            

La partie IDF calcule une fonction inversement proportionnelle à la fréquence du terme dans l'ensemble des documents (ou corpus). 

$$IDF(i) = \log{\frac{N_{d}}{f_{i}} + 1}$$

^ Expression       ^ Signification                                  ^
| $N_{d}$          | Nombre de documents                            |
| $f_{i}$          | Nombre de documents où le mot i apparaît        |

Enfin, 

$$TF(i,j) = TF_{i,j} * IDF_{i}$$

Ce score final permet de donner un score d'autant plus élevé que le terme est surreprésenté dans un document (par rapport à l'ensemble des documents).


**Pratique Python :**

On déclare et on entraine le modèle,

<code python>
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer()
tfs = tfidf.fit(dataTest['Commentaires'])
tfs = tfs.transform(dataTest['Commentaires'])
</code>


On construit un DataFrame (tableau de données) avec les différents scores calculés pour chaque mot. Sur les lignes on trouve les commentaires et dans les colonnes, chacun des mots du vocabulaire.

<code python>
import pandas as pd
feature_names = tfidf.get_feature_names()
corpus_index = [n for n in list(tfidf.vocabulary_.keys())]
df = pd.DataFrame(tfs.todense(), columns=feature_names)
</code>

On peut ainsi faire ressortir les phrases où un mot particulier aurait un score élevé. Prenons ici le mot "infirmière" :

<code python>
for x in df["infirmière"].sort_values(ascending =False).head(n=10).index : 
    print(dataTest.iloc[x]["Commentaires"])
</code>

**Résultat**

dit infirmière

vraiment infirmière

infirmière assistante techn

etre infirmière empêche dealer

cette infirmière caillasse policiers interpellée quoi plus normal pays normal maintenant va manifester

libérée autre part orange très vite empressée retirer prénom cette infirmière pourquoi parce disant

infirmière origine maghrébine faut surtout toucher laisser caillasser flics quel monde vivons


On remarque que les commentaires courts avec le mot cherché à l'intérieur remontent en premier ainsi que les commentaires un peu plus longs  mais avec plusieurs fois le mot cherché à l'intérieur.

**Pratique en R :**

Pour cette démonstration, nous utiliserons les romans de Jane Austen's. Rappelons, qu'ici aussi, il est bon de nettoyer le dataset des stopwords avant de l'étudier. 
Il faut, tout d'abord, construire un DataFrame avec les mots et leur roman associé :

<code Python>
library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)
</code>

^ Livre          ^ Mots          ^ Occurrences          ^
| Mansfield Park   | the           | 160 460             |
| Mansfield Park   | and           | 120 540             |
| Emma             | to            | 110 230             |
| Emma             | and           | 145 456             |

Ensuite, on peut appliquer l'algorithme de IF-IDF et visualiser les résultats :

<code Python>
bind_tf_idf(word, book, n)
book_words %>%
  select(-total) %>%
  
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()
</code>

**Résultat :**

{{ :cpp:tf-idfr.png?400 |}}

**Source**

[[https://www.tidytextmining.com/tfidf.html]]

[[https://www.ionos.fr/digitalguide/web-marketing/analyse-web/analyse-tf-idf/]]

=====Conclusion=====

 Tout au long de ce chapitre, nous avons découvert combien l'exploitation de données textuelles était complexe et diversifié. Il est nécessaire de bien respecter les étapes pour avoir une vision globale de la donnée et pouvoir, ainsi, mieux appréhender le problème soumis. Dans les prochaines pages, nous verrons comment classifier ce texte avec des modèles comme le LDA ou encore le Word Embedding se basant sur les réseaux de neurones.