{{ :cpp:biere.gif?direct&500 |}}

Pour illustrer ce cours, nous prendrons comme exemple la bière. Tout au long de sa fabrication , de son stockage ou de sa vente, la boisson est l'objet de multiples problèmes d'optimisation afin d'économiser des ressources et donc, générer plus de profits.
Le cours se déroulera selon le plan suivant :

  * [[cpp:Dichotomie| Méthode en une dimension ]]: recherche du zéro de la fonction gestion du stock de fûts:
            - Dichotomie
            - Regula Falsi
            - Newton

  * [[cpp:Gradient|Méthode de la Descente du gradient]] : recherche des conditions de pression et de température optimales pour une usure minimale de la cuve lors du houblonnage :
            - Méthode du gradient
            - Méthode du gradient conjugué


  * [[cpp:BFGS | Méthode de Broyden-Fletcher-Goldfarb-Shanno]]: recherche de la quantité minimale de stockage nécessaire en fonction du nombre d'acheteurs les n mois précèdents.


====Quels types de problèmes résolvons-nous ?====

Dans cette section, nous allons résoudre, à l'aide d'algorithmes,  des problèmes __d’optimisation__ et des problèmes de __résolution__. 

  *  Les problèmes d’optimisation sont des problèmes de recherche de minimums (ou maximums) locaux, c’est-à-dire :

;#;
$ x^{∗} = argmin (F(x), x ∈ \mathbb{R})$ où $F$ est une fonction de $\mathbb{R}^{d}$ dans $\mathbb{R}$. 
;#;

  * Les problèmes de résolution sont de la forme :
;#;
Trouver $x$ dans $\mathbb{R}^{d}$ solution de $f(x) = 0$ où $f$ est une fonction  de $\mathbb{R}^{d}$ dans $\mathbb{R}$.
;#;

__**Sources:**__ 
  *   [[https://www.ceremade.dauphine.fr/~gontier/Publications/methodesNumeriques.pdf]]
====Méthode analytique, méthode numérique====

Nous utiliserons des méthodes numériques qui donneront donc des résultats sous forme numérique et non sous forme analytique. 
Un résultat analytique correspond à  la forme exacte de la solution par exemple $ln(2)+exp(3*sin(4))$.  Pour ce même problème, le résultat numérique est ( à $10^{-4}$) près 0.7964. 

{{ :cpp:analytique.png?direct&400 |}}

Tous les algorithmes présentés ici sont itératifs. Il est donc nécessaire de comprendre le principe d'une boucle (informatique). La méthode globale consiste à partir d'un $x(0)$ quelconque et de calculer à chaque itération le terme suivant de la suite $x(n)$ qui converge vers $x^{*}$.

Il peut être intéressant de comparer les vitesses de convergence de chaque algorithme.  

__**Source:**__
  * [[http://maths.cnam.fr/IMG/pdf/Chapitre_III.pdf]]
====Les langages====

Dans ce cours, nous mettrons en parallèle deux langages.
Tout d'abord, nous verrons comment implémenter les algorithmes sous Python 3.6 qui présente le double avantage d'être facile à apprendre pour les débutants en informatique et d'être utilisé dans la plupart des entreprises. 

{{ :cpp:python.png?direct&200|}}
{{:cpp:octave.png?direct&200 |}}

Le deuxième langage utilisé pour ce cours sera Octave car il est compaptible à 98% avec Scilab et  Matlab (langage le plus communément utilisé pour les mathématiques, avec beaucoup de packages et une facilité d'écriture pour les matrices). Contrairement à Matlab, Octave est gratuit, ce qui le rend accessible à tous ceux qui veulent apprendre la programmation pour les sciences. 
\\
\\
\\
__**Source:**__
  * [[https://www.yourthesisadvisor.com/matlab-vs-octave-vs-scilab-vs-freemat/#:~:text=Octave%20is%2099%25%20compatible%20with%20MATLAB%2C%20and%20it%20is%20free.&text=While%20MATLAB's%20Window%20version%20is,or%20Octave's%20in%20most%20situations.]]

====Limites====

Il est important de noter que ces algorithmes ont tous des limites. Dans certains cas,  ils convergent lentement lorsque certains critères sur les fonctions sont respectés (l'algorithme de descente du gradient n'est fonctionnel que sur les fonctions convexes). 

Notons que les algorithmes en une dimension présentés ici sont simples et non optimisés. Bien sûr, Il existe des algorithmes plus efficaces comme, par exemple, la Méthode de Muller ou de Brent mais ils sont trop complexes pour une introduction à l'optimisation.

Voici les vitesses de convergence des algorithmes présentés ici:
\\
L: limite de ($x_n$)
\\
$\mu$: vitesse de convergence 

|                            ^  Complexité  ^  Expression mathématique de la complexité  ^
^  Dichotomie  |  Linéaire        |  $\lim\limits_{n \to \infty}\frac{\lvert x_{n+1} - L \rvert}{\lvert x_n - L \rvert}=\mu   \ (\mu >0) $  |
^  Newton  |  Quadratique  |  $\lim\limits_{n \to \infty}\frac{\lvert x_{n+1} - L \rvert}{\lvert x_n - L \rvert^2}=\mu   \  (\mu >0) $  |
^  Regula Falsi  |  Superlinéaire  |  $\lim\limits_{n \to \infty}\frac{\lvert x_{n+1} - L \rvert}{\lvert x_n - L \rvert}=\mu   \  (\mu =0) $  |
^  Gradient  |  Linéaire  |  $\lim\limits_{n \to \infty}\frac{\lvert x_{n+1} - L \rvert}{\lvert x_n - L \rvert}=\mu   \ (\mu >0) $  |
^  Gradient conjugué  |  Superlinéaire  |  $\lim\limits_{n \to \infty}\frac{\lvert x_{n+1} - L \rvert}{\lvert x_n - L \rvert}=\mu   \  (\mu =0) $  |
^  BFGS  |  Inconnu pour les fonctions non convexes  |  ???  |